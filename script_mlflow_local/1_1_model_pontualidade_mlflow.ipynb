{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5cd081-20db-47c3-9702-8a148f0fa6f6",
   "metadata": {},
   "source": [
    "# Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60625da5-0583-4d59-adc5-1980c0b1a38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.2.3\n",
      "NumPy version: 2.2.5\n",
      "XGBoost version: 3.0.0\n",
      "LightGBM version: 4.6.0\n",
      "SHAP version: 0.48.0\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas padrão e manipulação de dados\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime, date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytz import timezone\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Configurações e filtros\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Visualização de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import missingno as msno\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Machine Learning - Modelos e Pré-processamento\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, RepeatedStratifiedKFold, KFold, StratifiedKFold, GridSearchCV\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, average_precision_score, classification_report, confusion_matrix, f1_score,\n",
    "    log_loss, precision_recall_curve, precision_score, recall_score, roc_auc_score, roc_curve, auc,\n",
    "    balanced_accuracy_score, brier_score_loss, cohen_kappa_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_selection import (\n",
    "    VarianceThreshold, RFE, SelectFromModel, SequentialFeatureSelector, mutual_info_classif, mutual_info_regression\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Estatística e testes de hipótese\n",
    "from scipy.stats import (\n",
    "    chi2_contingency, kruskal, ks_2samp, fisher_exact, mannwhitneyu, power_divergence\n",
    ")\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Modelos avançados e otimização\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK, space_eval\n",
    "from hyperopt.pyll import scope\n",
    "from skopt import forest_minimize\n",
    "\n",
    "# Avaliação de modelos e explanação\n",
    "from shap import Explainer\n",
    "\n",
    "# Salvamento e carregamento de modelos com MLflow\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow.lightgbm\n",
    "import mlflow.catboost\n",
    "\n",
    "\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, balanced_accuracy_score, average_precision_score,\n",
    "    log_loss, brier_score_loss, cohen_kappa_score, matthews_corrcoef, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Impressão de versões das bibliotecas utilizadas\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "#print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"LightGBM version: {lgb.__version__}\")\n",
    "#print(f\"CatBoost version: {CatBoostClassifier.__module__.split('.')[0]} version: {ctb.__version__}\")\n",
    "print(f\"SHAP version: {shap.__version__}\")\n",
    "#print(f\"PPScore version: {pps.__version__}\")\n",
    "#print(f\"missingno version: {msno.__version__}\")\n",
    "#print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e9464-acc9-4965-a8bb-7d96fb4a47b7",
   "metadata": {},
   "source": [
    "# Criando ou carregando o experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b06438f-a6cf-4384-ac22-b50c638b20ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O experimento 'Dados aviação Aula' já existe.\n"
     ]
    }
   ],
   "source": [
    "# Nome do experimento que você deseja verificar/criar\n",
    "experiment_name = \"Dados aviação Aula\"\n",
    "\n",
    "# Verificar se o experimento já existe\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Se o experimento não existir, cria-o\n",
    "if experiment is None:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\"O experimento '{experiment_name}' foi criado.\")\n",
    "else:\n",
    "    print(f\"O experimento '{experiment_name}' já existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48bb255-be0c-4edd-9716-071c78de0fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O experimento id é:'559262076987350823'\n"
     ]
    }
   ],
   "source": [
    "# Verificar se o experimento já existe\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Id do experimento\n",
    "experiment_id = experiment.experiment_id\n",
    "print(f\"O experimento id é:'{experiment_id}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64091f7-8406-4fe7-9720-a2c8aeeed332",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7af7e6-74b2-4721-8388-f909e893d779",
   "metadata": {},
   "source": [
    "## Carregando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81700229-219d-4b6b-bc90-6d132e4a4ff7",
   "metadata": {},
   "source": [
    "## Desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398653a4-5ca1-40aa-bbc9-bd05ccd69017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o caminho do arquivo CSV que contém os dados históricos de voos.\n",
    "file_path = 'df_treinamento_2022_2023.csv'\n",
    "        \n",
    "# Lendo o arquivo CSV e carregando os dados em um DataFrame do pandas.\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce66f36-7f86-47d6-9f33-4247628a8733",
   "metadata": {},
   "source": [
    "## No mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765188ba-2498-469f-aadf-a8120e615e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:43:00 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start an MLflow run context\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name='extração e tratamento dos dados', \n",
    "                      description = 'Extração e/ou tratamento de dados',\n",
    "                      tags = {\"Extração\": \"origem_x\", \"objetivo\": \"alimentar o modelo_x\", \"Versão da etapa\": \"1.0\"}):\n",
    "    # Carregamento de dados históricos de voos a partir de um arquivo CSV.\n",
    "    # Definindo o caminho do arquivo CSV que contém os dados históricos de voos.\n",
    "    file_path = 'df_treinamento_2022_2023.csv'\n",
    "        \n",
    "    # Lendo o arquivo CSV e carregando os dados em um DataFrame do pandas.\n",
    "    df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd1211-adb5-412f-9868-dc01f94e5cdb",
   "metadata": {},
   "source": [
    "## Pre processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163e70c-3d31-4f99-80cb-82b38a0ea7ca",
   "metadata": {},
   "source": [
    "### Desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36e625-8a0c-4e33-850b-8bb67755fed5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns =[ 'codigo_di', 'codigo_tipo_linha'])\n",
    "\n",
    "df = df[list(df)]\n",
    "\n",
    "list_dummies =  colunas_categ = df.drop(columns = 'status_do_voo').select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Transformar colunas categóricas em tipo \"category\"\n",
    "df[list_dummies] = df[list_dummies].astype(\"category\")\n",
    "\n",
    "# Seleção das features preditoras (X) e variável-alvo (y)\n",
    "dt_ax = df.drop(columns=[\"status_do_voo\"])\n",
    "dt_ay = df[['status_do_voo']]\n",
    "\n",
    "# Codificação da variável-alvo\n",
    "label_mapping = {'Pontual': 0, 'Atrasado': 1}\n",
    "dt_ay = dt_ay['status_do_voo'].map(label_mapping)\n",
    "\n",
    "# Codifica colunas categóricas como inteiros\n",
    "label_encoders = {}\n",
    "for col in list_dummies:\n",
    "    le = LabelEncoder()\n",
    "    dt_ax[col] = le.fit_transform(dt_ax[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Segmentação em treino (86%) e teste (14,20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(dt_ax, dt_ay, random_state=33, test_size=0.142)\n",
    "\n",
    "# Segmentação adicional para validação/calibração (84,5% treino / 16,5% calibração)\n",
    "X_train_valid, X_test_valid, y_train_valid, y_test_valid = train_test_split(X_train, y_train, random_state=33, test_size=0.165)\n",
    "\n",
    "# Reverter os valores transformados para o tipo \"category\" original\n",
    "def revert_to_category(data, label_encoders, list_dummies):\n",
    "    for col in list_dummies:\n",
    "        if col in data.columns:\n",
    "            le = label_encoders[col]\n",
    "            data[col] = le.inverse_transform(data[col])\n",
    "    return data\n",
    "\n",
    "# Aplicar a reversão em X_smote_a, X_test_calib, X_test\n",
    "X_train_valid = revert_to_category(X_train_valid, label_encoders, list_dummies)\n",
    "X_test_valid = revert_to_category(X_test_valid, label_encoders, list_dummies)\n",
    "X_test = revert_to_category(X_test, label_encoders, list_dummies)\n",
    "\n",
    "# Para garantir que as colunas estão no tipo \"category\"\n",
    "X_train_valid[list_dummies] = X_train_valid[list_dummies].astype(\"category\")\n",
    "X_test_valid[list_dummies] = X_test_valid[list_dummies].astype(\"category\")\n",
    "X_test[list_dummies] = X_test[list_dummies].astype(\"category\")\n",
    "\n",
    "# Converte os nomes das colunas para uma lista de strings\n",
    "feature_names = list(X_test.columns)\n",
    "\n",
    "# Converte os conjuntos para DMatrix\n",
    "dtrain = xgb.DMatrix(X_train_valid, label=y_train_valid, enable_categorical=True, feature_names=feature_names, nthread=-1)\n",
    "dtest_valid = xgb.DMatrix(X_test_valid, label=y_test_valid, enable_categorical=True, feature_names=feature_names, nthread=-1)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True, feature_names=feature_names, nthread=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4966a-c2f3-411f-b273-62d93c73dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(X_train_valid.shape)\n",
    "print(X_test.shape)\n",
    "print(X_test_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25c094-e25b-4bfd-88d2-46229bf02160",
   "metadata": {},
   "source": [
    "### Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bfe8356-8f8f-4487-b42a-661c8cda5f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular e registrar a distribuição de classes\n",
    "def log_class_distribution(y, label):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    distribution = dict(zip(unique, counts))\n",
    "    total = sum(counts)\n",
    "    mlflow.log_param(f\"{label}_class_distribution\", {f\"Class {k}\": f\"{v/total:.2%}\" for k, v in distribution.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32fa4d6e-4552-4f78-a326-b15db87554fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=experiment_id, run_name='Pre-processamento',\n",
    "                      nested=True,\n",
    "                      description='Garantir o input correto dos modelos',\n",
    "                      tags={\"Pre-processamento\": \"preparação para treinamento\", \"objetivo\": \"garantir o input correto dos dados\", \"Versão da etapa\": \"1.0\"}):\n",
    "\n",
    "    # Etapa 1: Exclusão de colunas desnecessárias\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='drop columns', nested=True, \n",
    "                          description='Exclusão de colunas desnecessárias',\n",
    "                          tags={\"Tratamento\": \"drop_columns\"}):\n",
    "        df = df.drop(columns=['codigo_di', 'codigo_tipo_linha'])\n",
    "        mlflow.log_param(\"colunas_excluidas\", ['codigo_di', 'codigo_tipo_linha'])\n",
    "\n",
    "    # Etapa 2: Transformar colunas categóricas em tipo \"category\"\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Transformar colunas categóricas', nested=True, \n",
    "                          description='Converte colunas categóricas para o tipo category',\n",
    "                          tags={\"Tratamento\": \"category_conversion\"}):\n",
    "        list_dummies = df.drop(columns='status_do_voo').select_dtypes(include=['object']).columns.tolist()\n",
    "        df[list_dummies] = df[list_dummies].astype(\"category\")\n",
    "        mlflow.log_param(\"colunas_categoricas\", list_dummies)\n",
    "\n",
    "    # Etapa 3: Seleção de features e variável-alvo\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Seleção de features', nested=True, \n",
    "                          description='Selecionar features preditoras e variável-alvo',\n",
    "                          tags={\"Tratamento\": \"feature_selection\"}):\n",
    "        dt_ax = df.drop(columns=[\"status_do_voo\"])\n",
    "        dt_ay = df['status_do_voo'].map({'Pontual': 0, 'Atrasado': 1})\n",
    "        mlflow.log_param(\"target_mapping\", {'Pontual': 0, 'Atrasado': 1})\n",
    "        mlflow.log_param(\"n_features\", dt_ax.shape[1])\n",
    "\n",
    "    # Etapa 4: Codificação de colunas categóricas\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Codificação de colunas categóricas', nested=True, \n",
    "                          description='Codificar colunas categóricas como inteiros',\n",
    "                          tags={\"Tratamento\": \"label_encoding\"}):\n",
    "        label_encoders = {}\n",
    "        for col in list_dummies:\n",
    "            le = LabelEncoder()\n",
    "            dt_ax[col] = le.fit_transform(dt_ax[col])\n",
    "            label_encoders[col] = le\n",
    "        mlflow.log_param(\"n_label_encoded_columns\", len(list_dummies))\n",
    "\n",
    "    # Etapa 5: Segmentação em treino, teste e validação\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Segmentação em treino/teste/validação', nested=True, \n",
    "                          description='Segmentação dos dados em treino (71,64%), validação (14,15%) e teste (14,20%)',\n",
    "                          tags={\"Tratamento\": \"data_split\"}):\n",
    "        # Realizar a segmentação\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dt_ax, dt_ay, random_state=33, test_size=0.142)\n",
    "        X_train_valid, X_test_valid, y_train_valid, y_test_valid = train_test_split(X_train, y_train, random_state=33, test_size=0.165)\n",
    "        \n",
    "        # Registrar o tamanho dos conjuntos\n",
    "        mlflow.log_param(\"train_size\", len(X_train_valid))\n",
    "        mlflow.log_param(\"validation_size\", len(X_test_valid))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "        \n",
    "        # Registrar a distribuição de classes\n",
    "        log_class_distribution(y_train, 'train_size')\n",
    "        log_class_distribution(y_test_valid, 'validation_size')\n",
    "        log_class_distribution(y_test, 'test_size')\n",
    "\n",
    "    # Etapa 6: Reversão e preparação final dos dados\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Reversão e preparação final', nested=True, \n",
    "                          description='Reverter valores transformados para o tipo category original e preparação final',\n",
    "                          tags={\"Tratamento\": \"final_preparation\"}):\n",
    "        def revert_to_category(data, label_encoders, list_dummies):\n",
    "            for col in list_dummies:\n",
    "                if col in data.columns:\n",
    "                    le = label_encoders[col]\n",
    "                    data[col] = le.inverse_transform(data[col])\n",
    "            return data\n",
    "\n",
    "        X_train_valid = revert_to_category(X_train_valid, label_encoders, list_dummies)\n",
    "        X_test_valid = revert_to_category(X_test_valid, label_encoders, list_dummies)\n",
    "        X_test = revert_to_category(X_test, label_encoders, list_dummies)\n",
    "        \n",
    "        # Garantir que as colunas estão no tipo \"category\"\n",
    "        X_train_valid[list_dummies] = X_train_valid[list_dummies].astype(\"category\")\n",
    "        X_test_valid[list_dummies] = X_test_valid[list_dummies].astype(\"category\")\n",
    "        X_test[list_dummies] = X_test[list_dummies].astype(\"category\")\n",
    "        \n",
    "        mlflow.log_param(\"categorical_columns_finalized\", list_dummies)\n",
    "\n",
    "    # Etapa 7: Conversão para DMatrix\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Conversão para DMatrix', nested=True, \n",
    "                          description='Converter conjuntos de dados para DMatrix para treinamento com XGBoost',\n",
    "                          tags={\"Tratamento\": \"dmatrix_conversion\"}):\n",
    "        feature_names = list(X_test.columns)\n",
    "        dtrain = xgb.DMatrix(X_train_valid, label=y_train_valid, enable_categorical=True, feature_names=feature_names, nthread=-1)\n",
    "        dtest_valid = xgb.DMatrix(X_test_valid, label=y_test_valid, enable_categorical=True, feature_names=feature_names, nthread=-1)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True, feature_names=feature_names, nthread=-1)\n",
    "        mlflow.log_param(\"feature_names\", feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45978b-6be3-4718-8f9a-9fa72d03fcc0",
   "metadata": {},
   "source": [
    "## Hipertunnig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc21ad8-2034-4fdd-a37f-03df93490921",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hipertunnig desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800a2eb-abe4-4356-9040-605efac42cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "# Callback customizada que extende TrainingCallback e tem a assinatura correta.\n",
    "class FoldMetricsCallback(xgb.callback.TrainingCallback):\n",
    "    def __init__(self):\n",
    "        # Dicionário para armazenar os resultados por iteração.\n",
    "        # A chave é o número da iteração (int) e o valor é um dicionário com os resultados de cada fold.\n",
    "        self.fold_results = {}\n",
    "    \n",
    "    def after_iteration(self, model, epoch, evals):\n",
    "        iteration_results = {}\n",
    "        # No cv, o booster 'model' possui o atributo cvfolds\n",
    "        if hasattr(model, 'cvfolds'):\n",
    "            for i, cvpack in enumerate(model.cvfolds):\n",
    "                # Agora, passamos os argumentos feval=None e output_margin=False\n",
    "                result_str = cvpack.eval(epoch, feval=None, output_margin=False)\n",
    "                # Exemplo de result_str:\n",
    "                # \"train-auc:0.72862+0.00158  test-auc:0.68776+0.01517  test-aucpr:0.73000+0.00150  test-logloss:0.70000+0.01000  test-error:0.32000+0.02000\"\n",
    "                # Extraímos os valores numéricos das métricas de teste usando regex\n",
    "                matches = re.findall(r\"test-([\\w_]+):([\\d\\.]+)\", result_str)\n",
    "                for metric, value in matches:\n",
    "                    if metric in ['aucpr', 'auc', 'logloss', 'error']:\n",
    "                        key = f\"fold{i}_{metric}\"\n",
    "                        iteration_results[key] = float(value)\n",
    "        # Armazena os resultados da iteração 'epoch'\n",
    "        self.fold_results[epoch] = iteration_results\n",
    "        # Retorna False para continuar o treinamento\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9479ff4-c1e6-481e-8baf-1e7f8554cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hipertunnig(space):\n",
    "    \"\"\"\n",
    "    Realiza o ajuste de hiperparâmetros de um modelo XGBoost usando validação cruzada com DMatrix.\n",
    "    \n",
    "    Args:\n",
    "        space (dict): Dicionário contendo os hiperparâmetros avaliados pelo Hyperopt.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dicionário contendo o 'loss' (média do logloss do conjunto de CV), o 'status', \n",
    "              a 'best_iteration' e as métricas adicionais (média, mínimo e máximo de AUCPR e AUC).\n",
    "    \"\"\"\n",
    "    # Configuração dos parâmetros a partir do espaço definido\n",
    "    params = {\n",
    "        'max_depth': int(space['max_depth']),                    # Profundidade máxima da árvore\n",
    "        'gamma': space['gamma'],                                 # Redução mínima de perda para divisão de nó\n",
    "        'reg_alpha': space['reg_alpha'],                         # Regularização L1\n",
    "        'reg_lambda': space['reg_lambda'],                       # Regularização L2\n",
    "        'min_child_weight': int(space['min_child_weight']),      # Peso mínimo de instâncias em um nó filho\n",
    "        'colsample_bytree': space['colsample_bytree'],           # Proporção de colunas amostradas por árvore\n",
    "        'colsample_bylevel': space['colsample_bylevel'],         # Subamostragem de colunas por nível\n",
    "        'colsample_bynode': space['colsample_bynode'],           # Subamostragem de colunas por nó\n",
    "        'n_estimators': space['n_estimators'],                   # (Não utilizado pelo xgb.cv)\n",
    "        'learning_rate': space['learning_rate'],                 # Taxa de aprendizado\n",
    "        'max_delta_step': space['max_delta_step'],               # Limite de atualização das folhas\n",
    "        'subsample': space['subsample'],                         # Proporção de amostragem das instâncias\n",
    "        'sampling_method': space['sampling_method'],             # Método de amostragem\n",
    "        'tree_method': space['tree_method'],                     # Método de construção da árvore\n",
    "        'device': space['device'],                               # Dispositivo para treinamento (ex.: 'cuda')\n",
    "        'enable_categorical': space['enable_categorical'],       # Habilita suporte nativo a categóricas\n",
    "        'scale_pos_weight': space['scale_pos_weight'],           # Ajusta o peso das classes desbalanceadas\n",
    "        'eval_metric': space['eval_metric'],                     # Métricas de avaliação\n",
    "        'objective': space['objective'],                         # Função objetivo (ex.: 'binary:logistic')\n",
    "        'seed': space['seed'],                                   # Semente para reprodutibilidade\n",
    "        'max_cat_to_onehot': int(space['max_cat_to_onehot']),    # Limite para aplicar one-hot\n",
    "        'max_cat_threshold': int(space['max_cat_threshold']),    # Máximo de categorias para divisão\n",
    "        'max_leaves': int(space['max_leaves']),                  # Número máximo de folhas por árvore\n",
    "        'validate_parameters': space['validate_parameters'],     # Valida os parâmetros antes do treinamento\n",
    "        'max_bin': space['max_bin'],                             # Número máximo de bins para histogramas\n",
    "        'updater': space['updater']                              # Atualizador (ex.: 'grow_gpu_hist')\n",
    "    }\n",
    "\n",
    "    print(\"Hiperparâmetros utilizados:\", params)\n",
    "    print(\"Hiperparâmetros n_estimators:\", space['n_estimators'])\n",
    "    \n",
    "    # Instancia a callback customizada para extrair os resultados individuais por fold\n",
    "    fold_callback = FoldMetricsCallback()\n",
    "\n",
    "    # Realiza a validação cruzada com xgb.cv\n",
    "    cv_results = xgb.cv(\n",
    "        params=params,\n",
    "        dtrain=dtrain,                    # DMatrix preparado (global ou passado externamente)\n",
    "        num_boost_round=int(space['n_estimators']),\n",
    "        nfold=5,                          # Número de folds\n",
    "        metrics=[\"aucpr\", \"auc\", \"logloss\", \"error\"],  # Métricas de avaliação\n",
    "        as_pandas=True,                   # Retorna os resultados como DataFrame\n",
    "        seed=33,\n",
    "        stratified=True,                  # Garante estratificação dos folds\n",
    "        early_stopping_rounds=18 if params['max_depth'] <= 12 else 45,  # Early stopping condicional\n",
    "        verbose_eval=False,\n",
    "        callbacks=[fold_callback]\n",
    "    )\n",
    "\n",
    "    best_iteration = cv_results.shape[0]\n",
    "    \n",
    "    # Calcula estatísticas das métricas a partir do cv_results\n",
    "    max_aucpr = cv_results[\"test-aucpr-mean\"].max()\n",
    "    max_auc = cv_results[\"test-auc-mean\"].max()\n",
    "    max_logloss = cv_results[\"test-logloss-mean\"].max()\n",
    "\n",
    "    min_aucpr = cv_results[\"test-aucpr-mean\"].min()\n",
    "    min_auc = cv_results[\"test-auc-mean\"].min()\n",
    "    min_logloss = cv_results[\"test-logloss-mean\"].min()\n",
    "\n",
    "    mean_aucpr = cv_results[\"test-aucpr-mean\"].mean()\n",
    "    mean_auc = cv_results[\"test-auc-mean\"].mean()\n",
    "    mean_logloss = cv_results[\"test-logloss-mean\"].mean()\n",
    "\n",
    "    # Calcula estatísticas das métricas a partir do cv_results\n",
    "    max_error = cv_results[\"test-error-mean\"].max()\n",
    "    min_error = cv_results[\"test-error-mean\"].min()\n",
    "    mean_error = cv_results[\"test-error-mean\"].mean()\n",
    "\n",
    "\n",
    "     # Extração dos resultados individuais de cada fold usando os boosters retornados\n",
    "    fold_results = []\n",
    "\n",
    "\n",
    "    print(fold_results)\n",
    "\n",
    "    print(\"Melhor iteração:\", best_iteration)\n",
    "    print(\"Max AUCPR: \", max_aucpr)\n",
    "    print(\"Média AUCPR: \", mean_aucpr)\n",
    "    print(\"Min AUCPR: \", min_aucpr)\n",
    "    print(\"Max AUC: \", max_auc)\n",
    "    print(\"Média AUC: \", mean_auc)\n",
    "    print(\"Min AUC: \", min_auc)\n",
    "    print(\"Max LogLoss: \", max_logloss)\n",
    "    print(\"Média LogLoss: \", mean_logloss)\n",
    "    print(\"Min LogLoss: \", min_logloss)\n",
    "    print(\"Max error: \", max_error)\n",
    "    print(\"Média error: \", min_error)\n",
    "    print(\"Min error: \", mean_error)\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(\"Novo modelo\")\n",
    "    \n",
    "    # Seleciona os resultados individuais da iteração final (0-indexada)\n",
    "    selected_fold_results = fold_callback.fold_results.get(best_iteration - 1, {})\n",
    "\n",
    "    # Cria um dicionário com os resultados agregados e os resultados individuais por fold\n",
    "    result_dict = {\n",
    "        'loss': mean_logloss,\n",
    "        'best_iteration': best_iteration,\n",
    "        'mean_aucpr': mean_aucpr,\n",
    "        'max_aucpr': max_aucpr,\n",
    "        'min_aucpr': min_aucpr,\n",
    "        'mean_auc': mean_auc,\n",
    "        'max_auc': max_auc,\n",
    "        'min_auc': min_auc,\n",
    "        'mean_logloss': mean_logloss,\n",
    "        'max_logloss': max_logloss,\n",
    "        'min_logloss': min_logloss,\n",
    "        'mean_error': mean_error,\n",
    "        'max_error': max_error,\n",
    "        'min_error': min_error,\n",
    "        'fold_results': selected_fold_results, \n",
    "        'max_depth': int(space['max_depth']),                    # Profundidade máxima da árvore\n",
    "        'gamma': space['gamma'],                                 # Redução mínima de perda para divisão de nó\n",
    "        'reg_alpha': space['reg_alpha'],                         # Regularização L1\n",
    "        'reg_lambda': space['reg_lambda'],                       # Regularização L2\n",
    "        'min_child_weight': int(space['min_child_weight']),      # Peso mínimo de instâncias em um nó filho\n",
    "        'colsample_bytree': space['colsample_bytree'],           # Proporção de colunas amostradas por árvore\n",
    "        'colsample_bylevel': space['colsample_bylevel'],         # Subamostragem de colunas por nível\n",
    "        'colsample_bynode': space['colsample_bynode'],           # Subamostragem de colunas por nó\n",
    "        'n_estimators': space['n_estimators'],                   # (Não utilizado pelo xgb.cv)\n",
    "        'learning_rate': space['learning_rate'],                 # Taxa de aprendizado\n",
    "        'max_delta_step': space['max_delta_step'],               # Limite de atualização das folhas\n",
    "        'subsample': space['subsample'],                         # Proporção de amostragem das instâncias\n",
    "        'sampling_method': space['sampling_method'],             # Método de amostragem\n",
    "        'tree_method': space['tree_method'],                     # Método de construção da árvore\n",
    "        'device': space['device'],                               # Dispositivo para treinamento (ex.: 'cuda')\n",
    "        'enable_categorical': space['enable_categorical'],       # Habilita suporte nativo a categóricas\n",
    "        'scale_pos_weight': space['scale_pos_weight'],           # Ajusta o peso das classes desbalanceadas\n",
    "        'eval_metric': space['eval_metric'],                     # Métricas de avaliação\n",
    "        'objective': space['objective'],                         # Função objetivo (ex.: 'binary:logistic')\n",
    "        'seed': space['seed'],                                   # Semente para reprodutibilidade\n",
    "        'max_cat_to_onehot': int(space['max_cat_to_onehot']),    # Limite para aplicar one-hot\n",
    "        'max_cat_threshold': int(space['max_cat_threshold']),    # Máximo de categorias para divisão\n",
    "        'max_leaves': int(space['max_leaves']),                  # Número máximo de folhas por árvore\n",
    "        'validate_parameters': space['validate_parameters'],     # Valida os parâmetros antes do treinamento\n",
    "        'max_bin': space['max_bin'],                             # Número máximo de bins para histogramas\n",
    "        'updater': space['updater']                              # Atualizador (ex.: 'grow_gpu_hist')\n",
    "    }\n",
    "    \n",
    "    # Acumula o resultado na variável global\n",
    "    global global_results\n",
    "    global_results.append(result_dict)\n",
    "    \n",
    "    # Converte a lista acumulada em DataFrame e imprime uma parcial (últimos 5 resultados)\n",
    "    df_results = pd.DataFrame(global_results)\n",
    "    print(\"Partial results (últimos 10):\")\n",
    "    df_results.tail(10)\n",
    "    \n",
    "    # Retorna os resultados para o Hyperopt\n",
    "    return {\n",
    "        'loss': mean_logloss,  # Objetivo: minimizar o logloss\n",
    "        'status': STATUS_OK,\n",
    "        'best_iteration': best_iteration,\n",
    "        'additional_metrics': {\n",
    "            'mean_aucpr': mean_aucpr,\n",
    "            'mean_auc': mean_auc,\n",
    "            'mean_logloss': mean_logloss,\n",
    "            'max_aucpr': max_aucpr,\n",
    "            'max_auc': max_auc,\n",
    "            'max_logloss': max_logloss,\n",
    "            'min_aucpr': min_aucpr,\n",
    "            'min_auc': min_auc,\n",
    "            'min_logloss': min_logloss\n",
    "        },\n",
    "        'df_results': df_results  # Retorna o DataFrame com os resultados acumulados\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b78db7-31e6-4657-b5c1-bb41047f5c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    # Limita a profundidade das árvores para evitar modelos excessivamente complexos\n",
    "    'max_depth': scope.int(hp.quniform(\"max_depth\", 3, 30, 1)),\n",
    "    \n",
    "    # Gamma controla o ganho mínimo para realizar um split; um intervalo menor evita splits irrelevantes\n",
    "    'gamma': hp.uniform('gamma', 0, 18),\n",
    "    \n",
    "    # Regularização L1 para incentivar esparsidade; intervalo ajustado para não penalizar demais\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 120),\n",
    "    \n",
    "    # Regularização L2 para controlar coeficientes elevados\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 10),\n",
    "    \n",
    "    # Fração de features a serem usadas em cada árvore; intervalos que garantem diversidade entre as árvores\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 0.9),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.4, 1.0),\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.4, 1.0),\n",
    "    \n",
    "    # Peso mínimo necessário em um nó filho; evitar splits baseados em poucas instâncias\n",
    "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 1, 24, 1)),\n",
    "    \n",
    "    # Taxa de aprendizado em escala logarítmica para explorar valores pequenos sem pular demais\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n",
    "    \n",
    "    # Número de árvores; garante quantidade suficiente sem exagerar no overfitting\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 100, 1000, 10)),\n",
    "    \n",
    "    # Limita a magnitude da atualização dos nós para estabilidade no treinamento\n",
    "    'max_delta_step': hp.uniform('max_delta_step', 0, 10),\n",
    "    \n",
    "    # Subamostragem dos registros para cada árvore; ajuda a reduzir overfitting\n",
    "    'subsample': hp.uniform('subsample', 0.4, 1.0),\n",
    "    \n",
    "    # Método fixo de amostragem baseado em gradientes (já testado no seu contexto)\n",
    "    'sampling_method': 'gradient_based',\n",
    "    \n",
    "    # Utilizando o método de construção de árvores otimizado para GPU\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'device': 'cuda',\n",
    "    \n",
    "    # Ativa o tratamento nativo de variáveis categóricas\n",
    "    'enable_categorical': True,\n",
    "    \n",
    "    # Dado o desbalanceamento (razão aproximadamente 5:1), busca refinar o peso da classe positiva\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 3, 9),\n",
    "    \n",
    "    # Controle para uso de one-hot encoding apenas em variáveis de baixa cardinalidade (evita transformar \"numero_empresa_voo\")\n",
    "    'max_cat_to_onehot': scope.int(hp.quniform('max_cat_to_onehot', 3, 50, 1)),\n",
    "    'max_cat_threshold': scope.int(hp.quniform('max_cat_threshold', 3, 50, 1)),\n",
    "    \n",
    "    # Limita o número de folhas da árvore para controlar a complexidade\n",
    "    'max_leaves': scope.int(hp.quniform('max_leaves', 16, 256, 4)),\n",
    "    \n",
    "    # Validação dos parâmetros antes do início do treinamento\n",
    "    'validate_parameters': True,\n",
    "    \n",
    "    # Semente para garantir reprodutibilidade\n",
    "    'seed': 33,\n",
    "    \n",
    "    # Métricas utilizadas para avaliar a performance durante o treinamento\n",
    "    'eval_metric': [\"aucpr\", \"auc\", \"logloss\", \"error\"],\n",
    "    \n",
    "    # Atualizador otimizado para GPU\n",
    "    'updater': 'grow_gpu_hist',\n",
    "    \n",
    "    # Número máximo de bins para histogramas (afeta a granularidade dos splits)\n",
    "    'max_bin': scope.int(hp.quniform('max_bin', 32, 320, 8)),\n",
    "    \n",
    "    # Função objetivo para classificação binária\n",
    "    'objective': 'binary:logistic'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c447e-a019-41dc-9308-31e96161167d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Executando a otimização\n",
    "trials = Trials()\n",
    "# Lista global para acumular os resultados de cada trial\n",
    "global_results = []\n",
    "best_hyperparams = fmin(fn=hipertunnig, \n",
    "                        space=space, \n",
    "                        algo=tpe.suggest, \n",
    "                        max_evals=2, \n",
    "                        trials=trials,\n",
    "                        rstate=np.random.default_rng(42))  # Reprodutibilidade)\n",
    "\n",
    "# Obtendo os melhores hiperparâmetros\n",
    "best_hyperparams = space_eval(space, best_hyperparams)\n",
    "print(\"Melhores hiperparâmetros:\", best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c8fbc-0cb4-4ad6-af30-9ce2b83af780",
   "metadata": {},
   "source": [
    "### Hipertunning Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "400f49a4-f9a3-49a1-b431-a2efce7afc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    # Limita a profundidade das árvores para evitar modelos excessivamente complexos\n",
    "    'max_depth': scope.int(hp.quniform(\"max_depth\", 3, 30, 1)),\n",
    "    \n",
    "    # Gamma controla o ganho mínimo para realizar um split; um intervalo menor evita splits irrelevantes\n",
    "    'gamma': hp.uniform('gamma', 0, 18),\n",
    "    \n",
    "    # Regularização L1 para incentivar esparsidade; intervalo ajustado para não penalizar demais\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 120),\n",
    "    \n",
    "    # Regularização L2 para controlar coeficientes elevados\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 10),\n",
    "    \n",
    "    # Fração de features a serem usadas em cada árvore; intervalos que garantem diversidade entre as árvores\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 0.9),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.4, 1.0),\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.4, 1.0),\n",
    "    \n",
    "    # Peso mínimo necessário em um nó filho; evitar splits baseados em poucas instâncias\n",
    "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 1, 24, 1)),\n",
    "    \n",
    "    # Taxa de aprendizado em escala logarítmica para explorar valores pequenos sem pular demais\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n",
    "    \n",
    "    # Número de árvores; garante quantidade suficiente sem exagerar no overfitting\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 100, 1000, 10)),\n",
    "    \n",
    "    # Limita a magnitude da atualização dos nós para estabilidade no treinamento\n",
    "    'max_delta_step': hp.uniform('max_delta_step', 0, 10),\n",
    "    \n",
    "    # Subamostragem dos registros para cada árvore; ajuda a reduzir overfitting\n",
    "    'subsample': hp.uniform('subsample', 0.4, 1.0),\n",
    "    \n",
    "    # Método fixo de amostragem baseado em gradientes (já testado no seu contexto)\n",
    "    'sampling_method': 'gradient_based',\n",
    "    \n",
    "    # Utilizando o método de construção de árvores otimizado para GPU\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'device': 'cuda',\n",
    "    \n",
    "    # Ativa o tratamento nativo de variáveis categóricas\n",
    "    'enable_categorical': True,\n",
    "    \n",
    "    # Dado o desbalanceamento (razão aproximadamente 5:1), busca refinar o peso da classe positiva\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 3, 9),\n",
    "    \n",
    "    # Controle para uso de one-hot encoding apenas em variáveis de baixa cardinalidade (evita transformar \"numero_empresa_voo\")\n",
    "    'max_cat_to_onehot': scope.int(hp.quniform('max_cat_to_onehot', 3, 50, 1)),\n",
    "    'max_cat_threshold': scope.int(hp.quniform('max_cat_threshold', 3, 50, 1)),\n",
    "    \n",
    "    # Limita o número de folhas da árvore para controlar a complexidade\n",
    "    'max_leaves': scope.int(hp.quniform('max_leaves', 16, 256, 4)),\n",
    "    \n",
    "    # Validação dos parâmetros antes do início do treinamento\n",
    "    'validate_parameters': True,\n",
    "    \n",
    "    # Semente para garantir reprodutibilidade\n",
    "    'seed': 33,\n",
    "    \n",
    "    # Métricas utilizadas para avaliar a performance durante o treinamento\n",
    "    'eval_metric': [\"aucpr\", \"auc\", \"logloss\", \"error\"],\n",
    "    \n",
    "    # Atualizador otimizado para GPU\n",
    "    'updater': 'grow_gpu_hist',\n",
    "    \n",
    "    # Número máximo de bins para histogramas (afeta a granularidade dos splits)\n",
    "    'max_bin': scope.int(hp.quniform('max_bin', 32, 320, 8)),\n",
    "    \n",
    "    # Função objetivo para classificação binária\n",
    "    'objective': 'binary:logistic'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd681b2f-0397-4e2c-82cc-378362fcb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.xgboost.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "601f4a49-10d0-4aae-8276-524f1a480d1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Optional, Union\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    balanced_accuracy_score, average_precision_score, log_loss, brier_score_loss,\n",
    "    cohen_kappa_score, matthews_corrcoef, confusion_matrix\n",
    ")\n",
    "\n",
    "def calcular_metricas_binarias(\n",
    "    y_true: Union[np.ndarray, list],\n",
    "    y_pred_proba: Optional[Union[np.ndarray, list]] = None,\n",
    "    y_pred: Optional[Union[np.ndarray, list]] = None,\n",
    "    *,\n",
    "    threshold: float = 0.5,\n",
    "    sample_weight: Optional[Union[np.ndarray, list]] = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calcula métricas de classificação binária. Se apenas y_pred_proba for fornecido,\n",
    "    as classes são derivadas com base no threshold.\n",
    "\n",
    "    Parâmetros\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        Rótulos verdadeiros (binários).\n",
    "    y_pred_proba : array-like, opcional\n",
    "        Probabilidades da classe positiva (shape (n,) ou (n,2) – neste caso usa a coluna 1).\n",
    "    y_pred : array-like, opcional\n",
    "        Predições binárias (0/1). Se None, será calculado a partir de y_pred_proba e threshold.\n",
    "    threshold : float, padrão 0.5\n",
    "        Limiar para converter probabilidades em classes.\n",
    "    sample_weight : array-like, opcional\n",
    "        Pesos amostrais para métricas que suportam.\n",
    "\n",
    "    Retorno\n",
    "    -------\n",
    "    dict\n",
    "        Dicionário com métricas (acc, precision, recall, f1, auc, prauc, gini, logloss,\n",
    "        brier, kappa, mcc, balanced_accuracy, specificity, fpr, fnr, gmean, TN/FP/FN/TP).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "\n",
    "    if y_pred_proba is None and y_pred is None:\n",
    "        raise ValueError(\"Informe ao menos y_pred_proba ou y_pred.\")\n",
    "\n",
    "    # Probabilidades: aceita shape (n,) ou (n,2)\n",
    "    if y_pred_proba is not None:\n",
    "        y_pred_proba = np.asarray(y_pred_proba, dtype=float)\n",
    "        if y_pred_proba.ndim == 2 and y_pred_proba.shape[1] == 2:\n",
    "            y_pred_proba = y_pred_proba[:, 1]\n",
    "        # Clampa para [0,1] caso venha algo levemente fora (ex.: numérico instável)\n",
    "        y_pred_proba = np.clip(y_pred_proba, 0.0, 1.0)\n",
    "\n",
    "    # Se não vier y_pred, deriva pelo threshold\n",
    "    if y_pred is None:\n",
    "        y_pred = (y_pred_proba >= float(threshold)).astype(int)\n",
    "    else:\n",
    "        y_pred = np.asarray(y_pred).astype(int)\n",
    "\n",
    "    # Garante binarização 0/1 caso os rótulos não estejam exatamente assim\n",
    "    classes = np.unique(y_true)\n",
    "    if classes.size > 2:\n",
    "        raise ValueError(\"A função suporta apenas classificação binária.\")\n",
    "    if not np.array_equal(classes, np.array([0, 1])):\n",
    "        mapping = {classes[0]: 0, classes[-1]: 1}\n",
    "        y_true = np.vectorize(mapping.get)(y_true)\n",
    "\n",
    "    # Matriz de confusão 2x2 garantida\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1], sample_weight=sample_weight)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "    # Métricas baseadas em classes\n",
    "    accuracy = accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n",
    "    precision = precision_score(y_true, y_pred, sample_weight=sample_weight, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, sample_weight=sample_weight, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, sample_weight=sample_weight, zero_division=0)\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred, sample_weight=sample_weight)\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    fpr = FP / (FP + TN) if (FP + TN) > 0 else 0.0\n",
    "    fnr = FN / (FN + TP) if (FN + TP) > 0 else 0.0\n",
    "    gmean = float(np.sqrt(recall * specificity)) if (recall > 0 and specificity > 0) else 0.0\n",
    "\n",
    "    # Métricas que dependem de probabilidades\n",
    "    if y_pred_proba is not None and np.unique(y_true).size == 2:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_pred_proba, sample_weight=sample_weight)\n",
    "        except Exception:\n",
    "            auc = np.nan\n",
    "        try:\n",
    "            prauc = average_precision_score(y_true, y_pred_proba, sample_weight=sample_weight)\n",
    "        except Exception:\n",
    "            prauc = np.nan\n",
    "        gini = 2 * auc - 1 if not np.isnan(auc) else np.nan\n",
    "        try:\n",
    "            ll = log_loss(y_true, y_pred_proba, sample_weight=sample_weight, labels=[0, 1])\n",
    "        except Exception:\n",
    "            ll = np.nan\n",
    "        try:\n",
    "            brier = brier_score_loss(y_true, y_pred_proba, sample_weight=sample_weight)\n",
    "        except Exception:\n",
    "            brier = np.nan\n",
    "    else:\n",
    "        auc = prauc = gini = ll = brier = np.nan\n",
    "\n",
    "    # Outras métricas\n",
    "    try:\n",
    "        mcc = matthews_corrcoef(y_true, y_pred, sample_weight=sample_weight)\n",
    "    except Exception:\n",
    "        mcc = np.nan\n",
    "    try:\n",
    "        kappa = cohen_kappa_score(y_true, y_pred, sample_weight=sample_weight)\n",
    "    except Exception:\n",
    "        kappa = np.nan\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(threshold),\n",
    "        \"TN\": float(TN), \"FP\": float(FP), \"FN\": float(FN), \"TP\": float(TP),\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"balanced_accuracy\": float(balanced_acc),\n",
    "        \"specificity\": float(specificity),\n",
    "        \"fpr\": float(fpr),\n",
    "        \"fnr\": float(fnr),\n",
    "        \"gmean\": float(gmean),\n",
    "        \"auc\": float(auc) if not np.isnan(auc) else np.nan,\n",
    "        \"prauc\": float(prauc) if not np.isnan(prauc) else np.nan,\n",
    "        \"gini\": float(gini) if not np.isnan(gini) else np.nan,\n",
    "        \"logloss\": float(ll) if not np.isnan(ll) else np.nan,\n",
    "        \"brier\": float(brier) if not np.isnan(brier) else np.nan,\n",
    "        \"mcc\": float(mcc) if not np.isnan(mcc) else np.nan,\n",
    "        \"kappa\": float(kappa) if not np.isnan(kappa) else np.nan,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0575bfd-d77d-4695-b93a-55d9fcaf4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e65b5513-d55a-4c33-be56-c4e69667e443",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -------- 2) hipertunnig corrigida ----------\n",
    "import mlflow\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix\n",
    "\n",
    "def _to_eval_list(eval_metric):\n",
    "    \"\"\"Garante lista de strings para eval_metric.\"\"\"\n",
    "    if eval_metric is None:\n",
    "        return [\"logloss\"]\n",
    "    if isinstance(eval_metric, str):\n",
    "        return [eval_metric]\n",
    "    # tuple -> list\n",
    "    return list(eval_metric)\n",
    "\n",
    "def hipertunnig(space):\n",
    "    \"\"\"\n",
    "    Ajusta hiperparâmetros com xgb.cv, treina com o melhor num_boost_round\n",
    "    e loga tudo no MLflow. Usa calcular_metricas_binarias() para as métricas finais.\n",
    "    \"\"\"\n",
    "    # -------------------------\n",
    "    # Parâmetros\n",
    "    # -------------------------\n",
    "    params = {\n",
    "        'max_depth': int(space['max_depth']),\n",
    "        'gamma': space['gamma'],\n",
    "        'reg_alpha': space['reg_alpha'],\n",
    "        'reg_lambda': space['reg_lambda'],\n",
    "        'min_child_weight': int(space['min_child_weight']),\n",
    "        'colsample_bytree': space['colsample_bytree'],\n",
    "        'colsample_bylevel': space['colsample_bylevel'],\n",
    "        'colsample_bynode': space['colsample_bynode'],\n",
    "        'learning_rate': space['learning_rate'],\n",
    "        'max_delta_step': space['max_delta_step'],\n",
    "        'subsample': space['subsample'],\n",
    "        'sampling_method': space['sampling_method'],\n",
    "        'tree_method': space['tree_method'],\n",
    "        'device': space['device'],\n",
    "        'enable_categorical': space['enable_categorical'],\n",
    "        'scale_pos_weight': space['scale_pos_weight'],\n",
    "        'eval_metric': _to_eval_list(space.get('eval_metric', ['logloss'])),\n",
    "        'objective': space['objective'],\n",
    "        'seed': space['seed'],\n",
    "        'max_cat_to_onehot': int(space['max_cat_to_onehot']),\n",
    "        'max_cat_threshold': int(space['max_cat_threshold']),\n",
    "        'max_leaves': int(space['max_leaves']),\n",
    "        'validate_parameters': space['validate_parameters'],\n",
    "        'max_bin': space['max_bin'],\n",
    "        'updater': space['updater']\n",
    "    }\n",
    "    n_estimators = int(space['n_estimators'])\n",
    "\n",
    "    print(\"🔧 Hiperparâmetros utilizados:\", params)\n",
    "    print(\"🔢 n_estimators:\", n_estimators)\n",
    "\n",
    "    mlflow.xgboost.autolog()\n",
    "\n",
    "    with mlflow.start_run(experiment_id=experiment_id,\n",
    "                          run_name='XGBoost Model Training and Tuning',\n",
    "                          nested=True):\n",
    "\n",
    "        # -------------------------\n",
    "        # CV (usa metrics explícitas; ok manter também eval_metric no params)\n",
    "        # -------------------------\n",
    "        cv_results = xgb.cv(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=n_estimators,\n",
    "            nfold=5,\n",
    "            metrics=[\"aucpr\", \"auc\", \"logloss\"],  # ok em paralelo ao params['eval_metric']\n",
    "            as_pandas=True,\n",
    "            seed=33,\n",
    "            stratified=True,\n",
    "            early_stopping_rounds=15 if params['max_depth'] <= 6 else 45,\n",
    "        )\n",
    "\n",
    "        aucpr_list = cv_results[\"test-aucpr-mean\"].tolist()\n",
    "        auc_list = cv_results[\"test-auc-mean\"].tolist()\n",
    "        logloss_list = cv_results[\"test-logloss-mean\"].tolist()\n",
    "\n",
    "        mean_aucpr = max(aucpr_list)\n",
    "        mean_auc = max(auc_list)\n",
    "        mean_logloss = min(logloss_list)\n",
    "\n",
    "        # best rounds pela parada antecipada (tamanho do DF)\n",
    "        best_num_boost_round = cv_results.shape[0]\n",
    "\n",
    "        mlflow.log_metric(\"mean_aucpr\", mean_aucpr)\n",
    "        mlflow.log_metric(\"mean_auc\", mean_auc)\n",
    "        mlflow.log_metric(\"mean_logloss\", mean_logloss)\n",
    "        mlflow.log_metric(\"best_num_boost_round\", best_num_boost_round)\n",
    "\n",
    "        # -------------------------\n",
    "        # Treinamento final com o best_num_boost_round\n",
    "        # -------------------------\n",
    "        evals_list = []\n",
    "        try:\n",
    "            evals_list = [(dtest_valid, 'validation')]\n",
    "        except NameError:\n",
    "            # se não existir dtest_valid, avalia só em treino\n",
    "            evals_list = [(dtrain, 'train')]\n",
    "\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=best_num_boost_round,\n",
    "            evals=evals_list,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # Predições e métricas (test)\n",
    "        # -------------------------\n",
    "        y_pred_proba = booster.predict(dtest)\n",
    "        metrics_test = calcular_metricas_binarias(y_test, y_pred_proba, threshold=0.5)\n",
    "\n",
    "        for k, v in metrics_test.items():\n",
    "            mlflow.log_metric(f\"test_{k}\", v)\n",
    "\n",
    "        # (Opcional) Métricas em treino também\n",
    "        y_train_true = dtrain.get_label()\n",
    "        y_train_proba = booster.predict(dtrain)\n",
    "        metrics_train = calcular_metricas_binarias(y_train_true, y_train_proba, threshold=0.5)\n",
    "        for k, v in metrics_train.items():\n",
    "            mlflow.log_metric(f\"train_{k}\", v)\n",
    "\n",
    "        # -------------------------\n",
    "        # Artefatos (gráficos)\n",
    "        # -------------------------\n",
    "        # Matriz de confusão (test)\n",
    "        cm = confusion_matrix(y_test, (y_pred_proba >= 0.5).astype(int))\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "        plt.title('Matriz de Confusão (Test)')\n",
    "        plt.xlabel('Predito')\n",
    "        plt.ylabel('Real')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png')\n",
    "        mlflow.log_artifact('confusion_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Importância das features\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        xgb.plot_importance(booster, max_num_features=20)\n",
    "        plt.title('Importância das Features')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png')\n",
    "        mlflow.log_artifact('feature_importance.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Curva ROC\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, linestyle='--', label=f\"AUC = {metrics_test.get('auc', float('nan')):.3f}\")\n",
    "        plt.title('Curva ROC (Test)')\n",
    "        plt.xlabel('FPR')\n",
    "        plt.ylabel('TPR')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('roc_curve.png')\n",
    "        mlflow.log_artifact('roc_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Curva Precisão-Recall\n",
    "        prec_vals, rec_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(rec_vals, prec_vals, marker='.', label=f\"PRAUC = {metrics_test.get('prauc', float('nan')):.3f}\")\n",
    "        plt.title('Curva Precisão-Recall (Test)')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precisão')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('precision_recall_curve.png')\n",
    "        mlflow.log_artifact('precision_recall_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "        # -------------------------\n",
    "        # Log final do modelo\n",
    "        # -------------------------\n",
    "        y_train_proba = booster.predict(dtrain)\n",
    "\n",
    "        # Log the model with signature\n",
    "        from mlflow.models import infer_signature\n",
    "    \n",
    "        signature = infer_signature(X_train, y_train_proba)\n",
    "        \n",
    "        mlflow.xgboost.log_model(\n",
    "        xgb_model=booster,\n",
    "        name=\"modelo_xgboost\",\n",
    "        signature=signature,\n",
    "        input_example=X_train[:5],\n",
    "        )\n",
    "        \n",
    "        #mlflow.xgboost.log_model(\n",
    "        #    xgb_model=booster,\n",
    "        #    artifact_path=\"modelo_xgboost\",\n",
    "            #model_format=\"json\"\n",
    "        #)\n",
    "\n",
    "        return {\n",
    "            'loss': mean_logloss,\n",
    "            'status': STATUS_OK,\n",
    "            'cv_metrics': {'aucpr': mean_aucpr, 'auc': mean_auc, 'logloss': mean_logloss},\n",
    "            'best_num_boost_round': best_num_boost_round,\n",
    "            'final_metrics_test': metrics_test,\n",
    "            'final_metrics_train': metrics_train\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f87b2fd-e2a7-41c0-be37-cd1928d5d94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 8, 'gamma': 11.044241278530942, 'reg_alpha': 100.68540694740837, 'reg_lambda': 4.4204962722439225, 'min_child_weight': 15, 'colsample_bytree': 0.5209846933141805, 'colsample_bylevel': 0.9160740581284995, 'colsample_bynode': 0.8290919036015767, 'learning_rate': 0.018770532351419632, 'max_delta_step': 9.77163723980769, 'subsample': 0.7825629817802873, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 4.861562748338326, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 12, 'max_cat_threshold': 18, 'max_leaves': 168, 'validate_parameters': True, 'max_bin': 88, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "640                                                                                                                    \n",
      "  0%|                                                                           | 0/60 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:44:38 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 01:44:38 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 01:44:42 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|########5                                                   | 1/7 [00:00<00:00, 996.98it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|#################1                                          | 2/7 [00:00<00:00, 999.71it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################7                                  | 3/7 [00:00<00:00, 999.52it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|##################################2                         | 4/7 [00:00<00:00, 999.95it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 833.10it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 857.12it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 874.91it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 777.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 10, 'gamma': 12.45293556443537, 'reg_alpha': 1.978885898505176, 'reg_lambda': 6.6220690987869055, 'min_child_weight': 18, 'colsample_bytree': 0.7132686256766255, 'colsample_bylevel': 0.9993658610997173, 'colsample_bynode': 0.6735222961715663, 'learning_rate': 0.04404362431826378, 'max_delta_step': 7.6413780978565065, 'subsample': 0.9840071027472885, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 3.6137530533652162, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 5, 'max_cat_threshold': 13, 'max_leaves': 252, 'validate_parameters': True, 'max_bin': 240, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "150                                                                                                                    \n",
      "  2%|▊                                              | 1/60 [01:32<1:31:21, 92.91s/trial, best loss: 0.5805111508143176]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:45:32 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 01:45:32 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 01:45:35 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|########4                                                  | 1/7 [00:00<00:00, 1000.07it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|################8                                          | 2/7 [00:00<00:00, 1000.07it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################2                                 | 3/7 [00:00<00:00, 1000.07it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|##################################2                         | 4/7 [00:00<00:00, 799.56it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 833.06it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 856.94it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 874.88it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 699.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 12, 'gamma': 11.756685794606776, 'reg_alpha': 108.07600094808781, 'reg_lambda': 1.0634819074577218, 'min_child_weight': 10, 'colsample_bytree': 0.4963438901324335, 'colsample_bylevel': 0.7502326742069667, 'colsample_bynode': 0.7648021298814314, 'learning_rate': 0.2391116231755152, 'max_delta_step': 9.603142654610625, 'subsample': 0.5158091156735369, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 6.387601532324096, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 23, 'max_cat_threshold': 29, 'max_leaves': 36, 'validate_parameters': True, 'max_bin': 96, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "280                                                                                                                    \n",
      "  3%|█▌                                             | 2/60 [02:25<1:06:36, 68.90s/trial, best loss: 0.4981773477884296]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:46:07 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 01:46:07 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 01:46:10 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|#########8                                                           | 1/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|################8                                          | 2/7 [00:00<00:00, 1000.19it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################7                                  | 3/7 [00:00<00:00, 999.60it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|##################################2                         | 4/7 [00:00<00:00, 999.66it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 999.88it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 857.18it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 874.85it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 777.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 7, 'gamma': 12.854619950226688, 'reg_alpha': 100.4211711279482, 'reg_lambda': 5.100654065557717, 'min_child_weight': 4, 'colsample_bytree': 0.6819004216821365, 'colsample_bylevel': 0.8723354800081827, 'colsample_bynode': 0.9878452516644881, 'learning_rate': 0.03291994022432445, 'max_delta_step': 3.0105203852280003, 'subsample': 0.7795747125075339, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 6.125705466633934, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 34, 'max_cat_threshold': 36, 'max_leaves': 184, 'validate_parameters': True, 'max_bin': 216, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "240                                                                                                                    \n",
      "  5%|██▍                                              | 3/60 [02:59<50:30, 53.16s/trial, best loss: 0.4981773477884296]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:46:49 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 01:46:49 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 01:46:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|#########8                                                           | 1/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|################8                                          | 2/7 [00:00<00:00, 1998.72it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################2                                 | 3/7 [00:00<00:00, 1499.57it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|#################################7                         | 4/7 [00:00<00:00, 1000.07it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 999.98it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 999.95it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|###########################################################| 7/7 [00:00<00:00, 1000.00it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 874.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 28, 'gamma': 9.460262526728503, 'reg_alpha': 63.383861249390556, 'reg_lambda': 9.590325989774048, 'min_child_weight': 3, 'colsample_bytree': 0.8692009884906202, 'colsample_bylevel': 0.7068812826539119, 'colsample_bynode': 0.894596994047149, 'learning_rate': 0.084837248475267, 'max_delta_step': 9.866597575123262, 'subsample': 0.8905881848445374, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 3.5744746019568057, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 20, 'max_cat_threshold': 42, 'max_leaves': 56, 'validate_parameters': True, 'max_bin': 152, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "500                                                                                                                    \n",
      "  7%|███▎                                             | 4/60 [03:41<45:31, 48.78s/trial, best loss: 0.4981773477884296]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:47:30 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 01:47:30 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 01:47:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|########5                                                   | 1/7 [00:00<00:00, 998.41it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|#################1                                          | 2/7 [00:00<00:00, 998.52it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################7                                  | 3/7 [00:00<00:00, 999.83it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|##################################2                         | 4/7 [00:00<00:00, 999.83it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 833.13it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 856.94it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 874.93it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 777.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 13, 'gamma': 9.824908308441135, 'reg_alpha': 92.38232617733807, 'reg_lambda': 5.718960107664239, 'min_child_weight': 15, 'colsample_bytree': 0.6164474439742667, 'colsample_bylevel': 0.8918165893955093, 'colsample_bynode': 0.43494673175851145, 'learning_rate': 0.11764744601886551, 'max_delta_step': 0.20300035585466447, 'subsample': 0.5680448235172076, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 3.1535133394468833, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 23, 'max_cat_threshold': 35, 'max_leaves': 36, 'validate_parameters': True, 'max_bin': 216, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "800                                                                                                                    \n",
      "  8%|████                                             | 5/60 [04:22<42:12, 46.04s/trial, best loss: 0.4981773477884296]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:48:20 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 01:48:20 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 01:48:23 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|#########8                                                           | 1/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|#################1                                          | 2/7 [00:00<00:00, 999.83it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################7                                  | 3/7 [00:00<00:00, 999.91it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|##################################2                         | 4/7 [00:00<00:00, 932.43it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 945.05it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 953.83it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 844.29it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 753.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 14, 'gamma': 1.430630150990322, 'reg_alpha': 113.66343267141534, 'reg_lambda': 5.751142835663084, 'min_child_weight': 7, 'colsample_bytree': 0.4320761802123466, 'colsample_bylevel': 0.539530580784046, 'colsample_bynode': 0.7626688451931545, 'learning_rate': 0.2874384092859929, 'max_delta_step': 0.08904153167743512, 'subsample': 0.4624320957600463, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 3.69773402321234, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 3, 'max_cat_threshold': 20, 'max_leaves': 56, 'validate_parameters': True, 'max_bin': 48, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "250                                                                                                                    \n",
      " 10%|████▊                                           | 6/60 [05:13<42:48, 47.57s/trial, best loss: 0.48740899275613253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:49:08 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 01:49:08 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 01:49:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|#########8                                                           | 1/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|################8                                          | 2/7 [00:00<00:00, 2000.62it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################2                                 | 3/7 [00:00<00:00, 1000.23it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|##################################2                         | 4/7 [00:00<00:00, 999.95it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 999.41it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|##################################################5        | 6/7 [00:00<00:00, 1000.03it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|###########################################################| 7/7 [00:00<00:00, 1000.00it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 874.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 15, 'gamma': 4.7290718673327845, 'reg_alpha': 5.542334515855245, 'reg_lambda': 8.266203077884922, 'min_child_weight': 17, 'colsample_bytree': 0.8234660474133497, 'colsample_bylevel': 0.6847895299711599, 'colsample_bynode': 0.5899316926411781, 'learning_rate': 0.03265256761091956, 'max_delta_step': 5.907831238561627, 'subsample': 0.4741920949826416, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 4.985790127723398, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 43, 'max_cat_threshold': 39, 'max_leaves': 176, 'validate_parameters': True, 'max_bin': 248, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "920                                                                                                                    \n",
      " 12%|█████▌                                          | 7/60 [06:00<41:52, 47.40s/trial, best loss: 0.48740899275613253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:54:01 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 01:54:01 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 01:54:04 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|#########8                                                           | 1/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|################8                                          | 2/7 [00:00<00:00, 2000.62it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################7                                  | 3/7 [00:00<00:00, 999.83it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|##################################2                         | 4/7 [00:00<00:00, 999.54it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 982.18it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 846.51it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 266.52it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 247.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 19, 'gamma': 4.968709241579504, 'reg_alpha': 89.27309696767857, 'reg_lambda': 7.860043935419244, 'min_child_weight': 24, 'colsample_bytree': 0.6680316528444721, 'colsample_bylevel': 0.8270883789266384, 'colsample_bynode': 0.43298486184671037, 'learning_rate': 0.07454840953628435, 'max_delta_step': 9.43438522340667, 'subsample': 0.9793391431278602, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 7.4598309776753045, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 37, 'max_cat_threshold': 25, 'max_leaves': 172, 'validate_parameters': True, 'max_bin': 64, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "560                                                                                                                    \n",
      " 13%|██████                                       | 8/60 [10:56<1:49:38, 126.51s/trial, best loss: 0.48740899275613253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:55:20 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 01:55:20 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 01:55:22 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|#########8                                                           | 1/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|#################1                                          | 2/7 [00:00<00:00, 797.09it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################7                                  | 3/7 [00:00<00:00, 854.99it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|##################################2                         | 4/7 [00:00<00:00, 886.93it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 907.50it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 921.83it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 876.45it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 778.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 14, 'gamma': 17.50456378393796, 'reg_alpha': 116.86081926610929, 'reg_lambda': 5.832709577999802, 'min_child_weight': 3, 'colsample_bytree': 0.8392436041971018, 'colsample_bylevel': 0.5167244113984667, 'colsample_bynode': 0.8493306588245927, 'learning_rate': 0.2777149100052217, 'max_delta_step': 2.918091072786085, 'subsample': 0.4110635243411179, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 4.183081723849615, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 45, 'max_cat_threshold': 42, 'max_leaves': 92, 'validate_parameters': True, 'max_bin': 224, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "710                                                                                                                    \n",
      " 15%|██████▊                                      | 9/60 [12:12<1:34:08, 110.75s/trial, best loss: 0.48740899275613253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:55:55 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 01:55:55 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 01:55:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|#########8                                                           | 1/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|#################1                                          | 2/7 [00:00<00:00, 999.60it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################7                                  | 3/7 [00:00<00:00, 854.82it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|##################################2                         | 4/7 [00:00<00:00, 886.46it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 906.92it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 921.22it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 822.21it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 735.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 7, 'gamma': 13.926721807580567, 'reg_alpha': 54.571715572949124, 'reg_lambda': 0.15264213142296756, 'min_child_weight': 14, 'colsample_bytree': 0.7663794262768111, 'colsample_bylevel': 0.6522365536212138, 'colsample_bynode': 0.756726730423994, 'learning_rate': 0.09698653117594778, 'max_delta_step': 7.256893367215822, 'subsample': 0.9721712130100555, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 7.138280408792858, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 5, 'max_cat_threshold': 35, 'max_leaves': 160, 'validate_parameters': True, 'max_bin': 232, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "140                                                                                                                    \n",
      " 17%|███████▌                                     | 10/60 [12:47<1:12:54, 87.49s/trial, best loss: 0.48740899275613253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:56:26 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 01:56:26 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 01:56:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|#########8                                                           | 1/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|################8                                          | 2/7 [00:00<00:00, 2003.49it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################2                                 | 3/7 [00:00<00:00, 1499.93it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|##################################2                         | 4/7 [00:00<00:00, 886.89it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 906.95it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 921.62it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 822.37it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 735.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 17, 'gamma': 2.7443347797869775, 'reg_alpha': 96.23690887875082, 'reg_lambda': 6.294283269454221, 'min_child_weight': 15, 'colsample_bytree': 0.48424614647132286, 'colsample_bylevel': 0.8927764864535322, 'colsample_bynode': 0.5699658160278829, 'learning_rate': 0.19216404001291204, 'max_delta_step': 5.818118554198143, 'subsample': 0.6217048619057225, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 7.611580526691185, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 9, 'max_cat_threshold': 6, 'max_leaves': 252, 'validate_parameters': True, 'max_bin': 208, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "530                                                                                                                    \n",
      " 18%|████████▌                                      | 11/60 [13:18<57:18, 70.17s/trial, best loss: 0.48740899275613253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:58:02 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 01:58:02 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 01:58:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|#########8                                                           | 1/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|################8                                          | 2/7 [00:00<00:00, 1000.91it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################2                                 | 3/7 [00:00<00:00, 1000.47it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|#################################7                         | 4/7 [00:00<00:00, 1000.19it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################1                | 5/7 [00:00<00:00, 1000.17it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 857.29it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 875.25it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 772.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 9, 'gamma': 11.68259904878954, 'reg_alpha': 70.21686414097826, 'reg_lambda': 7.333279413093128, 'min_child_weight': 6, 'colsample_bytree': 0.5980361700072745, 'colsample_bylevel': 0.42928221219450063, 'colsample_bynode': 0.5036835460711266, 'learning_rate': 0.19045653308276939, 'max_delta_step': 1.3515858397573566, 'subsample': 0.4064188876311967, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 6.843380726127015, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 26, 'max_cat_threshold': 34, 'max_leaves': 48, 'validate_parameters': True, 'max_bin': 64, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "610                                                                                                                    \n",
      " 20%|█████████                                    | 12/60 [14:56<1:02:54, 78.63s/trial, best loss: 0.48740899275613253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 01:58:54 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 01:58:54 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 01:58:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|#########8                                                           | 1/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|################8                                          | 2/7 [00:00<00:00, 1000.31it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################2                                 | 3/7 [00:00<00:00, 1000.23it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|##################################2                         | 4/7 [00:00<00:00, 911.21it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 927.70it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 939.02it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 947.22it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 834.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 24, 'gamma': 14.791857638700664, 'reg_alpha': 113.60292017174356, 'reg_lambda': 4.200690025333742, 'min_child_weight': 9, 'colsample_bytree': 0.6062837639446688, 'colsample_bylevel': 0.5614328812529829, 'colsample_bynode': 0.6614566195664692, 'learning_rate': 0.04822149726789656, 'max_delta_step': 3.58571763250888, 'subsample': 0.6672135351041404, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 7.572915053632915, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 21, 'max_cat_threshold': 20, 'max_leaves': 236, 'validate_parameters': True, 'max_bin': 176, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "870                                                                                                                    \n",
      " 22%|██████████▏                                    | 13/60 [15:48<55:12, 70.48s/trial, best loss: 0.48740899275613253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 02:00:12 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 02:00:12 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 02:00:15 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|########4                                                  | 1/7 [00:00<00:00, 1000.79it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|#################1                                          | 2/7 [00:00<00:00, 798.31it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################7                                  | 3/7 [00:00<00:00, 855.40it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|##################################2                         | 4/7 [00:00<00:00, 887.40it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 907.78it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 922.06it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 932.39it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 822.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 21, 'gamma': 12.07934295496145, 'reg_alpha': 49.80995424043088, 'reg_lambda': 7.106801922263984, 'min_child_weight': 22, 'colsample_bytree': 0.4114712324431084, 'colsample_bylevel': 0.5074442365186821, 'colsample_bynode': 0.7741273949496197, 'learning_rate': 0.1564188512594234, 'max_delta_step': 7.83107885852388, 'subsample': 0.5248647429146472, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 3.210143358736889, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 25, 'max_cat_threshold': 13, 'max_leaves': 216, 'validate_parameters': True, 'max_bin': 56, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "670                                                                                                                    \n",
      " 23%|██████████▉                                    | 14/60 [17:05<55:33, 72.46s/trial, best loss: 0.48740899275613253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 02:01:00 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 02:01:00 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 02:01:03 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|########4                                                  | 1/7 [00:00<00:00, 1000.07it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|################8                                          | 2/7 [00:00<00:00, 1000.07it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################7                                  | 3/7 [00:00<00:00, 856.10it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|##################################2                         | 4/7 [00:00<00:00, 725.91it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 767.99it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 798.86it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 735.94it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 665.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 8, 'gamma': 13.03988525656461, 'reg_alpha': 7.3964262542294446, 'reg_lambda': 7.349689167615615, 'min_child_weight': 1, 'colsample_bytree': 0.6229575241374989, 'colsample_bylevel': 0.5282952213008865, 'colsample_bynode': 0.49039405438823735, 'learning_rate': 0.04401470213982398, 'max_delta_step': 8.708326093470784, 'subsample': 0.9121224164435766, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 6.9497717496821165, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 43, 'max_cat_threshold': 32, 'max_leaves': 196, 'validate_parameters': True, 'max_bin': 240, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "780                                                                                                                    \n",
      " 25%|███████████▊                                   | 15/60 [17:53<48:56, 65.26s/trial, best loss: 0.48740899275613253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 02:02:19 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 02:02:19 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 02:02:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n",
      "Downloading artifacts:   0%|                                                                     | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  14%|#########8                                                           | 1/7 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  29%|#################1                                          | 2/7 [00:00<00:00, 998.41it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  43%|#########################2                                 | 3/7 [00:00<00:00, 1000.07it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  57%|#################################7                         | 4/7 [00:00<00:00, 1000.19it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  71%|##########################################8                 | 5/7 [00:00<00:00, 828.88it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts:  86%|###################################################4        | 6/7 [00:00<00:00, 853.28it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 871.56it/s]\n",
      "\u001b[A\n",
      "Downloading artifacts: 100%|############################################################| 7/7 [00:00<00:00, 774.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Hiperparâmetros utilizados:                                                                                         \n",
      "{'max_depth': 10, 'gamma': 13.861334608067802, 'reg_alpha': 0.48486459670456394, 'reg_lambda': 8.731594991196527, 'min_child_weight': 19, 'colsample_bytree': 0.766425768810145, 'colsample_bylevel': 0.726583315980482, 'colsample_bynode': 0.9423716305938303, 'learning_rate': 0.02136707519851313, 'max_delta_step': 5.134966209012673, 'subsample': 0.7136050715155553, 'sampling_method': 'gradient_based', 'tree_method': 'gpu_hist', 'device': 'cuda', 'enable_categorical': True, 'scale_pos_weight': 3.3784463152221154, 'eval_metric': ['aucpr', 'auc', 'logloss', 'error'], 'objective': 'binary:logistic', 'seed': 33, 'max_cat_to_onehot': 28, 'max_cat_threshold': 19, 'max_leaves': 128, 'validate_parameters': True, 'max_bin': 176, 'updater': 'grow_gpu_hist'}\n",
      "🔢 n_estimators:                                                                                                       \n",
      "500                                                                                                                    \n",
      " 27%|████████████▌                                  | 16/60 [19:19<52:15, 71.26s/trial, best loss: 0.48740899275613253]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/10 02:04:12 WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.\n",
      "\n",
      "2025/09/10 02:04:12 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\n",
      "2025/09/10 02:04:15 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Etapa de hipertuning\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name='Hipertunning XGBoost', nested=True,\n",
    "                      description='Busca pelos melhores parâmetros. Os modelos testados são armazenados, mesmo que não tenham os melhores parâmetros.',\n",
    "                      tags={\"Hipertunning\": \"Melhores parâmetros\", \"objetivo\": \"garantir os melhores parâmetros para o modelo\"}):\n",
    "    \n",
    "    # Executando a otimização\n",
    "    trials = Trials()\n",
    "    # Lista global para acumular os resultados de cada trial\n",
    "    global_results = []\n",
    "    best_hyperparams = fmin(fn=hipertunnig, \n",
    "                            space=space, \n",
    "                            algo=tpe.suggest, \n",
    "                            max_evals=60, \n",
    "                            trials=trials,\n",
    "                            rstate=np.random.default_rng(40))  # Reprodutibilidade)\n",
    "    \n",
    "    # Obtendo os melhores hiperparâmetros\n",
    "    best_hyperparams = space_eval(space, best_hyperparams)\n",
    "    print(\"Melhores hiperparâmetros:\", best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c32e1b-a5ac-4357-bd6d-6489b776c651",
   "metadata": {},
   "source": [
    "## Treinamento final do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5542e296-cd33-424c-878c-729bbb4adf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'max_depth': int(best_hyperparams['max_depth']),\n",
    "    'n_estimators': int(best_hyperparams['n_estimators']),\n",
    "    'reg_lambda': float(best_hyperparams['reg_lambda']),\n",
    "    'reg_alpha': float(best_hyperparams['reg_alpha']),\n",
    "    'gamma': float(best_hyperparams['gamma']),\n",
    "    'min_child_weight': int(best_hyperparams['min_child_weight']),\n",
    "    'colsample_bytree': float(best_hyperparams['colsample_bytree']),\n",
    "    'colsample_bylevel': float(best_hyperparams['colsample_bylevel']),\n",
    "    'colsample_bynode': float(best_hyperparams['colsample_bynode']),\n",
    "    'learning_rate': float(best_hyperparams['learning_rate']),\n",
    "    'max_delta_step': float(best_hyperparams.get('max_delta_step', 0.0)),\n",
    "    'subsample': float(best_hyperparams['subsample']),\n",
    "    'sampling_method': best_hyperparams.get('sampling_method', 'gradient_based'),\n",
    "    'tree_method': best_hyperparams.get('tree_method', 'hist'),\n",
    "    'scale_pos_weight': float(best_hyperparams['scale_pos_weight']),\n",
    "    'max_cat_to_onehot': int(best_hyperparams.get('max_cat_to_onehot', 10)),\n",
    "    'max_cat_threshold': int(best_hyperparams.get('max_cat_threshold', 20)),\n",
    "    'max_leaves': int(best_hyperparams.get('max_leaves', 256)),\n",
    "    'max_bin': int(best_hyperparams.get('max_bin', 256)),\n",
    "    'updater': best_hyperparams.get('updater', 'grow_gpu_hist'),\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': [\"aucpr\", \"auc\"],\n",
    "    'enable_categorical': True,\n",
    "    'validate_parameters': True,\n",
    "    'seed': int(best_hyperparams.get('seed', 33)),\n",
    "    'device': best_hyperparams.get('device', 'cuda'),\n",
    "    'verbosity': 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a399db-a011-4c8d-b601-6b23a28a0899",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.xgboost.autolog()\n",
    "with mlflow.start_run(\n",
    "    experiment_id=experiment_id,\n",
    "    run_name='Treinamento e avaliação XGBoost',\n",
    "    description='Treinamento com melhores hiperparâmetros e avaliação do modelo final',\n",
    "    tags={\"Tipo\": \"Classificação\", \"Modelo\": \"XGBoost\", \"Etapa\": \"Treinamento final\"}):\n",
    "    \n",
    "    \n",
    "    # Log dos parâmetros do modelo\n",
    "    mlflow.log_params(best_params)\n",
    "    \n",
    "    num_boost_round = int(best_hyperparams['n_estimators'])\n",
    "    # Treinamento do modelo\n",
    "    model_class = xgb.train(\n",
    "        params=best_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        evals=[(dtest_valid, 'validation')],\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=False)\n",
    "    \n",
    "    # Previsões\n",
    "    y_pred_proba = model_class.predict(dtest)\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Métricas de desempenho\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1_score\": f1_score(y_test, y_pred),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "        \"auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"prauc\": average_precision_score(y_test, y_pred_proba),\n",
    "        \"mcc\": matthews_corrcoef(y_test, y_pred),\n",
    "        \"log_loss\": log_loss(y_test, y_pred_proba),\n",
    "        \"brier_score\": brier_score_loss(y_test, y_pred_proba),\n",
    "        \"cohen_kappa\": cohen_kappa_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "    # Log de métricas individualmente\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "    \n",
    "    # Gráficos e artefatos\n",
    "    # Matriz de Confusão\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    mlflow.log_artifact('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Importância das Features\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    xgb.plot_importance(model_class, max_num_features=20)\n",
    "    plt.title('Importância das Features')\n",
    "    plt.savefig('feature_importance.png')\n",
    "    mlflow.log_artifact('feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Curva ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, linestyle='--', label='Curva ROC (AUC = {:.3f})'.format(metrics[\"auc\"]))\n",
    "    plt.title('Curva ROC')\n",
    "    plt.xlabel('Taxa de Falsos Positivos')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "    plt.legend()\n",
    "    plt.savefig('roc_curve.png')\n",
    "    mlflow.log_artifact('roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Curva de Precisão-Recall\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall_vals, precision_vals, marker='.', label='PRAUC = {:.3f}'.format(metrics[\"prauc\"]))\n",
    "    plt.title('Curva de Precisão-Recall')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precisão')\n",
    "    plt.legend()\n",
    "    plt.savefig('precision_recall_curve.png')\n",
    "    mlflow.log_artifact('precision_recall_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    y_train_proba = model_class.predict(dtrain)\n",
    "\n",
    "    # Log the model with signature\n",
    "    from mlflow.models import infer_signature\n",
    "    \n",
    "    signature = infer_signature(X_train, y_train_proba)\n",
    "        \n",
    "    mlflow.xgboost.log_model(\n",
    "        xgb_model=model_class,\n",
    "        name=\"modelo_xgboost_final\",\n",
    "        signature=signature,\n",
    "        input_example=X_train[:5],\n",
    "        )\n",
    "\n",
    "    print(\"Treinamento e logging concluídos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a7060-10b1-4817-97d5-6c71d6ca156e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Alteernativa 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a704f4e-d38b-4f38-a962-a1c8fa4d9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=experiment_id, run_name='Pre-processamento',\n",
    "                      run_id='ba596ce0c0ff43228f85f3ef932a8310',\n",
    "                      nested=True,\n",
    "                      description = 'Garantir o input correto dos modelos',\n",
    "                      tags = {\"Pre-processamento\": \"preparação para treinamento\", \"objetivo\": \"garantir o input correto dos dados\", \"Versão da etapa\": \"1.0\"}):\n",
    "    \n",
    "   \n",
    "    \n",
    "    dft = df[chosen_columns].sample(frac=0.3, random_state=13)\n",
    "    \n",
    "    # Colunas que precisam passar por one hot encoding\n",
    "    list_dummies = ['nome_empresas','codigo_tipo_linha','descricao_origem','descricao_destino','pais_origem','pais_destino','continente_origem',\n",
    "                 'continente_destino','cidade_origem','cidade_destino','uf_origem','uf_destino','mes_partida',\n",
    "                 'dia_semana_chegada']\n",
    "\n",
    "    final_data = pd.DataFrame()\n",
    "    # Logar os parâmetros\n",
    "    mlflow.log_param(\"Colunas escolhidas\", chosen_columns)\n",
    "    mlflow.log_param(\"Index\", 'num_cpf')\n",
    "    mlflow.log_param(\"Colunas para one-hot encoding\", list_dummies)\n",
    "    \n",
    "    # Logar métricas\n",
    "    mlflow.log_metric(\"Quantidade de colunas\", len(chosen_columns))\n",
    "    mlflow.log_metric(\"Quantidade de colunas dummies\", len(list_dummies))\n",
    "    mlflow.log_metric(\"Quantidade de colunas não dummies\", len(chosen_columns) - len(list_dummies) - 1) \n",
    "    \n",
    "    ### One hot encoding\n",
    "    with mlflow.start_run(experiment_id=experiment_id, nested=True, run_name='One hot encoding', run_id='8836439277bc460e8767f9e6b7311883',\n",
    "                      description = 'Transformação das colunas categoricas em númericas',\n",
    "                      tags = {\"One hot encoding\": \"Transformar categorica em númerica\", \"objetivo\": \"garantir o input correto dos dados\", \"Versão da etapa\": \"1.0\"}):\n",
    "        for column in list_dummies:\n",
    "            encoder = preprocessing.OneHotEncoder(handle_unknown='ignore')\n",
    "            encoder.fit(dft[[column]])\n",
    "            \n",
    "            # Logar parâmetros para cada coluna processada\n",
    "            mlflow.log_param(f\"Coluna_{column.lower()}\", column.lower())\n",
    "            \n",
    "            enc_df = pd.DataFrame(encoder.transform(dft[[column]]).toarray(), \n",
    "                                  columns=encoder.get_feature_names_out([column]))\n",
    "            final_data = pd.concat([final_data, enc_df], axis=1)\n",
    "\n",
    "        final_data['status_do_voo'] = dft['status_do_voo'].values\n",
    "\n",
    "        dt_ax = final_data.drop(columns=[\"status_do_voo\"])\n",
    "        dt_ay = final_data[['status_do_voo']].copy()\n",
    "\n",
    "        # Transformação da coluna em valores binarios. Pontual = 1 e Atrasado = 0\n",
    "        label_encoder = LabelEncoder()\n",
    "        dt_ay_enc = label_encoder.fit_transform(dt_ay)\n",
    "        dt_ay_df = pd.DataFrame(dt_ay_enc, columns=dt_ay.columns)\n",
    "\n",
    "        # Suponha que 'df' é o seu DataFrame\n",
    "        column_names = dt_ax.columns.tolist()\n",
    "        name_map = clean_column_names(column_names)\n",
    "        \n",
    "        # Renomear colunas no DataFrame\n",
    "        dt_ax.rename(columns=name_map, inplace=True)\n",
    "        \n",
    "    ### Normalização / Segmentação  treino e teste / Smote\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Normalização e Smote', nested=True,run_id='a058314be1ff4283b8fafd1168611eba',\n",
    "                      description = 'Implementação da etapa de normalização e SMOTE dos dados. Essas etapas são essenciais para evitar overfiting e underfitting',\n",
    "                      tags = {\"Normalização e SMOTE\": \"Normalização em range de 0 a 1 e criação de dados sinteticos para balencear\", \"objetivo\": \"garantir qualidade no correto dos dados\", \"Versão da etapa\": \"1.0\"}):\n",
    "        # Normalização dos dados\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(dt_ax)\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=dt_ax.columns)\n",
    "    \n",
    "        # Segmentação em Treino (85%) e Teste (15%)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, dt_ay_df, random_state=13, test_size=0.15)\n",
    "    \n",
    "        # Logar distribuição das classes antes do SMOTE\n",
    "        log_class_distribution(y_test, 'original')\n",
    "    \n",
    "        # Aplicar SMOTE\n",
    "        smote = SMOTE(random_state=13)\n",
    "        X_smote_a, y_smote_a = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        X_test = X_test.reset_index().drop(columns = 'index')\n",
    "        y_test = y_test.reset_index().drop(columns = 'index')\n",
    "    \n",
    "        # Logar distribuição das classes após SMOTE\n",
    "        log_class_distribution(y_smote_a, 'SMOTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24da70-4c24-483d-b232-33ce8df2ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_hyper_tuning(space):\n",
    "    \"\"\"\n",
    "    Realiza o ajuste de hiperparâmetros e treinamento de um modelo XGBoost com logging completo utilizando MLflow.\n",
    "    \n",
    "    Args:\n",
    "        space (dict): Dicionário contendo os hiperparâmetros para o modelo XGBoost.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dicionário contendo o 'loss' (negativo da média do AUC) e o 'status'.\n",
    "    \"\"\"\n",
    "    mlflow.xgboost.autolog()\n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='Unified Model Training and Tuning', nested=True):\n",
    "        #  Configuração do modelo com os parâmetros do espaço\n",
    "        clf = xgb.XGBClassifier(max_depth = space['max_depth'],\n",
    "                                  learning_rate = space['learning_rate'],\n",
    "                                  reg_alpha = space['reg_alpha'],\n",
    "                                  reg_lambda = space['reg_lambda'],\n",
    "                                  min_child_weight = space['min_child_weight'],\n",
    "                                  subsample = space['subsample'],\n",
    "                                  colsample_bytree = space['colsample_bytree'],\n",
    "                                  gamma = space['gamma'],\n",
    "                                  objective = space['objective'],\n",
    "                                  seed = space['seed'])\n",
    "        \n",
    "        # StratifiedKFold para manter a proporção de classes em cada fold\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Avaliação usando cross_val_score no conjunto de treinamento\n",
    "        auc_scores = cross_val_score(clf, X_train, y_train, cv=skf, scoring='roc_auc')\n",
    "        mean_auc = auc_scores.mean()\n",
    "\n",
    "        # Logando a média do AUC\n",
    "        mlflow.log_metric('mean_auc', mean_auc)\n",
    "        \n",
    "        model = clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Teste do modelo\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "        # Teste do modelo e log das curvas\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "        # Plotar e salvar a Curva de Precisão-Recall\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall, precision, marker='.')\n",
    "        plt.title('Curva de Precisão-Recall')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precisão')\n",
    "        plt.savefig('precision_recall_curve.png')\n",
    "        plt.close()\n",
    "    \n",
    "        # Plotar e salvar a Curva ROC\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, linestyle='--')\n",
    "        plt.title('Curva ROC')\n",
    "        plt.xlabel('Taxa de Falso Positivo')\n",
    "        plt.ylabel('Taxa de Verdadeiro Positivo')\n",
    "        plt.savefig('roc_curve.png')\n",
    "        plt.close()\n",
    "    \n",
    "        # Logar gráficos como artefatos\n",
    "        mlflow.log_artifact('precision_recall_curve.png')\n",
    "        mlflow.log_artifact('roc_curve.png')\n",
    "\n",
    "        # Create a model signature\n",
    "        signature = infer_signature(X_test, model.predict(X_test))\n",
    "        model_info = mlflow.xgboost.log_model(model, \"modelo_xgboost\", signature=signature) \n",
    "        \n",
    "        mlflow.xgboost.log_model(model, \"model_xgb\", signature=signature)\n",
    "        model_uri = mlflow.get_artifact_uri(\"model_xgb\")\n",
    "        \n",
    "        eval_data = pd.DataFrame(X_test, columns=dt_ax.columns)\n",
    "        eval_data['atraso30_m3'] = y_test.reset_index(drop=True)\n",
    "        \n",
    "        result = mlflow.evaluate(model_uri,\n",
    "                                 eval_data,\n",
    "                                 targets=\"atraso30_m3\",\n",
    "                                 model_type=\"classifier\",\n",
    "                                 evaluators=[\"default\"])\n",
    "\n",
    "        # A função de perda é o negativo da média do AUC para otimização\n",
    "        return {'loss': -mean_auc, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "  'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "  'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "  'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "  'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "  'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "  'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "  'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "  'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "  'objective': 'binary:logistic',\n",
    "  'seed': 123, # Set a seed for deterministic training\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866616d-12d0-45fe-bf8c-ccbd16a1ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etapa de hipertunning\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name='Hipertunnig', nested=True,\n",
    "                      description = 'Busca pelos melhores parametros. Os modelos testados são armazenados, mesmo que não tenha os melhores parametros.',\n",
    "                      tags = {\"Hipertunnig\": \"Melhores parametros\", \"objetivo\": \"garantir os melhores parametros para o modelo\", \"Versão da etapa\": \"1.0\"}):\n",
    "    # Executando a otimização\n",
    "    trials = Trials()\n",
    "    best_hyperparams = fmin(fn=unified_hyper_tuning, \n",
    "                            space=space, \n",
    "                            algo=tpe.suggest, \n",
    "                            max_evals=5, \n",
    "                            trials=trials)\n",
    "    \n",
    "    # Obtendo os melhores hiperparâmetros\n",
    "    mlflow.log_params(best_hyperparams)\n",
    "    best_hyperparams = space_eval(space, best_hyperparams)\n",
    "    print(\"Melhores hiperparâmetros:\", best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f5c8de-3ef0-46b3-9208-d89d4a27aa75",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5da9b1-1c67-442a-8e71-41f4e8a384c1",
   "metadata": {},
   "source": [
    "## Criando ou carregando o experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bfa196-9c02-495a-ad44-5e74002f2e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Nome do experimento que você deseja verificar/criar\n",
    "experiment_name = \"CatBoost MLflow Aviação\"\n",
    "\n",
    "# Verificar se o experimento já existe\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Se o experimento não existir, cria-o\n",
    "if experiment is None:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\"O experimento '{experiment_name}' foi criado.\")\n",
    "else:\n",
    "    print(f\"O experimento '{experiment_name}' já existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b608913-c200-4f80-aeee-b8c93bbe5605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verificar se o experimento já existe\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Id do experimento\n",
    "experiment_id = experiment.experiment_id\n",
    "print(f\"O experimento id é:'{experiment_id}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16e347-23e3-4a19-8c1b-6c041a533bd4",
   "metadata": {},
   "source": [
    "## Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533b7e6-e30a-4356-9b04-fc11acf22af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=experiment_id,\n",
    "    run_name=\"Pipeline de Pré-processamento CatBoost\", \n",
    "                      description=\"Pipeline completo para preparação de dados históricos de voos\",\n",
    "                      tags={\"Etapa\": \"Pipeline de Pre-processamento\", \"versão\": \"1.0\"}):\n",
    "\n",
    "    # Etapa 1: Carregamento dos dados tratados\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Carregando dataset tratado\", nested=True):\n",
    "        # Lendo os dados\n",
    "        file_path = 'df_treinamento_2022_2023.csv'\n",
    "        df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Etapa 2: Exclusão de colunas desnecessárias\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Exclusão de Colunas\", nested=True):\n",
    "        df = df.drop(columns=['codigo_di', 'codigo_tipo_linha'])\n",
    "        mlflow.log_param(\"colunas_excluidas\", ['codigo_di', 'codigo_tipo_linha'])\n",
    "\n",
    "    # Etapa 3: Identificação de colunas categóricas\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Identificação de Categóricas\", nested=True):\n",
    "        list_dummies = df.drop(columns='status_do_voo').select_dtypes(include=['object']).columns.tolist()\n",
    "        mlflow.log_param(\"colunas_categoricas\", list_dummies)\n",
    "\n",
    "    # Etapa 4: Seleção de features e variável-alvo\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Seleção de Features\", nested=True):\n",
    "        dt_ax = df.drop(columns=[\"status_do_voo\"])\n",
    "        dt_ay = df['status_do_voo'].map({'Pontual': 0, 'Atrasado': 1})\n",
    "        mlflow.log_param(\"target_mapping\", {'Pontual': 0, 'Atrasado': 1})\n",
    "        mlflow.log_param(\"n_features\", dt_ax.shape[1])\n",
    "\n",
    "    # Etapa 5: Codificação de colunas categóricas\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Codificação de Categóricas\", nested=True):\n",
    "        label_encoders = {}\n",
    "        for col in list_dummies:\n",
    "            le = LabelEncoder()\n",
    "            dt_ax[col] = le.fit_transform(dt_ax[col])\n",
    "            label_encoders[col] = le\n",
    "        mlflow.log_param(\"n_label_encoded_columns\", len(list_dummies))\n",
    "\n",
    "    # Etapa 6: Segmentação em treino, teste e validação\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Segmentação dos Dados\", nested=True):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dt_ax, dt_ay, random_state=33, test_size=0.142)\n",
    "        X_train_valid, X_test_valid, y_train_valid, y_test_valid = train_test_split(X_train, y_train, random_state=33, test_size=0.165)\n",
    "\n",
    "        mlflow.log_param(\"train_size\", len(X_train_valid))\n",
    "        mlflow.log_param(\"validation_size\", len(X_test_valid))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "\n",
    "    # Etapa 7: Reversão e finalização das colunas categóricas\n",
    "    with mlflow.start_run(experiment_id=experiment_id,run_name=\"Reversão de Colunas Categóricas\", nested=True):\n",
    "        def revert_to_category(data, label_encoders, list_dummies):\n",
    "            for col in list_dummies:\n",
    "                if col in data.columns:\n",
    "                    le = label_encoders[col]\n",
    "                    data[col] = le.inverse_transform(data[col])\n",
    "            return data\n",
    "\n",
    "        X_train_valid = revert_to_category(X_train_valid, label_encoders, list_dummies)\n",
    "        X_test_valid = revert_to_category(X_test_valid, label_encoders, list_dummies)\n",
    "        X_test = revert_to_category(X_test, label_encoders, list_dummies)\n",
    "\n",
    "        mlflow.log_param(\"categorical_columns_finalized\", list_dummies)\n",
    "\n",
    "    # Etapa 7: Resumo do Pipeline\n",
    "    mlflow.log_param(\"pipeline_status\", \"Concluído\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36bbe5-ebb4-4a72-967f-98a4e1cb1da2",
   "metadata": {},
   "source": [
    "## Treinamento sem hipertuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4504182e-8a2f-44cb-9b40-f4e55baad229",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193b4fba-27c1-4ffb-a937-5bdb0e79433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with mlflow.start_run(\n",
    "    experiment_id=experiment_id,\n",
    "    run_name='Treinamento e avaliação CatBoost',\n",
    "    nested=True,\n",
    "    description='Treinamento com melhores hiperparâmetros e avaliação do modelo final',\n",
    "    tags={\"Tipo\": \"Classificação\", \"Modelo\": \"CatBoost\", \"Etapa\": \"Treinamento final\"}):\n",
    "    \n",
    "    # Log dos parâmetros do modelo\n",
    "    mlflow.log_params(best_params)\n",
    "    \n",
    "    # Configuração do modelo CatBoostClassifier\n",
    "    classifier_params = best_params.copy()\n",
    "    cat_features = list(X_train_valid.select_dtypes(include=['object']))\n",
    "    model = CatBoostClassifier(\n",
    "        cat_features=cat_features, \n",
    "        eval_metric='AUC')\n",
    "\n",
    "    \n",
    "    # Treinamento do modelo\n",
    "    model.fit(X_train_valid, y_train_valid, \n",
    "              eval_set=(X_test_valid, y_test_valid), \n",
    "              cat_features=cat_features, \n",
    "              verbose=100,\n",
    "              plot=True)\n",
    "    \n",
    "    # Previsões\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "     # Métricas de desempenho\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "    fnr = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "    g_mean = np.sqrt(sensitivity * specificity)\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall (sensibilidade)\": recall_score(y_test, y_pred),\n",
    "        \"f1_score\": f1_score(y_test, y_pred),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "        \"specificity\": specificity,\n",
    "        \"auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"prauc\": average_precision_score(y_test, y_pred_proba),\n",
    "        \"mcc\": matthews_corrcoef(y_test, y_pred),\n",
    "        \"log_loss\": log_loss(y_test, y_pred_proba),\n",
    "        \"brier_score\": brier_score_loss(y_test, y_pred_proba),\n",
    "        \"cohen_kappa\": cohen_kappa_score(y_test, y_pred),\n",
    "        \"false_positive_rate (FPR)\": fpr,\n",
    "        \"false_negative_rate (FNR)\": fnr,\n",
    "        \"geometric_mean (G-Mean)\": g_mean\n",
    "    }\n",
    "\n",
    "\n",
    "    # Log de métricas individualmente\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "    \n",
    "    # Gráficos e artefatos\n",
    "    # Matriz de Confusão\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    mlflow.log_artifact('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Curva ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, linestyle='--', label='Curva ROC (AUC = {:.3f})'.format(metrics[\"auc\"]))\n",
    "    plt.title('Curva ROC')\n",
    "    plt.xlabel('Taxa de Falsos Positivos')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "    plt.legend()\n",
    "    plt.savefig('roc_curve.png')\n",
    "    mlflow.log_artifact('roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Curva de Precisão-Recall\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall_vals, precision_vals, marker='.', label='PRAUC = {:.3f}'.format(metrics[\"prauc\"]))\n",
    "    plt.title('Curva de Precisão-Recall')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precisão')\n",
    "    plt.legend()\n",
    "    plt.savefig('precision_recall_curve.png')\n",
    "    mlflow.log_artifact('precision_recall_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    # SHAP Importance\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_test)\n",
    "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    sorted_idx = shap_importance.argsort()\n",
    "\n",
    "    # Gráfico de importância SHAP\n",
    "    fig = plt.figure(figsize=(7, 7))\n",
    "    plt.barh(range(len(sorted_idx)), shap_importance[sorted_idx], align='center')\n",
    "    plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])\n",
    "    plt.title('SHAP Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_importance.png')\n",
    "    mlflow.log_artifact('shap_importance.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Beeswarm SHAP\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.plots.beeswarm(shap_values, max_display=15, show=False)\n",
    "    plt.title('SHAP Beeswarm')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_beeswarm.png')\n",
    "    mlflow.log_artifact('shap_beeswarm.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Registrar o modelo no MLflow\n",
    "    signature = infer_signature(X_test, y_pred_proba)\n",
    "    mlflow.catboost.log_model(\n",
    "        model,\n",
    "        artifact_path=\"model_catboost\",\n",
    "        signature=signature\n",
    "    )\n",
    "    \n",
    "    print(\"Treinamento, logging e gráfico SHAP concluídos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde854e-0931-4eab-a401-aaa40aa4022f",
   "metadata": {},
   "source": [
    "## Hipertuning Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52f72f-fdba-4cb1-a841-6eef9f6ad2da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hipertuning desenvolvimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323ee2a5-440c-4099-8773-ff33e9018a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "space_catboost = {\n",
    "    'iterations': scope.int(hp.quniform('iterations', 100, 1000, 50)),   # Número de árvores\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, -0.3),           # Taxa de aprendizado (0.05 ~ 0.7)\n",
    "    'depth': scope.int(hp.quniform('depth', 4, 12, 1)),                  # Profundidade da árvore (controle de overfitting)\n",
    "    'l2_leaf_reg': hp.loguniform('l2_leaf_reg', -3, 2),                  # Regularização L2 (1 ~ 100)\n",
    "    'random_strength': hp.uniform('random_strength', 0, 2),              # Aleatoriedade nas divisões\n",
    "    'bagging_temperature': hp.uniform('bagging_temperature', 0, 1),      # Temperatura para amostragem de dados\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 4, 8),            # Peso para classes desbalanceadas\n",
    "    'min_data_in_leaf': scope.int(hp.quniform('min_data_in_leaf', 10, 100, 10)),  # Mínimo de dados por folha\n",
    "    'max_bin': scope.int(hp.quniform('max_bin', 128, 256, 32)),          # Número máximo de bins\n",
    "    'grow_policy': hp.choice('grow_policy', ['Depthwise', 'Lossguide']), # Política de crescimento\n",
    "    'eval_metric': 'AUC',                                                # Métrica de avaliação\n",
    "    'task_type': 'GPU',                                                  # Utilizar GPU\n",
    "    'random_seed': 42                                                    # Reprodutibilidade\n",
    "}\n",
    "\n",
    "\n",
    "# Função objetivo para o Hyperopt\n",
    "def objective(params):\n",
    "    params['eval_metric'] = params['eval_metric']  # Define a métrica de avaliação\n",
    "    params['loss_function'] = 'Logloss'           # Objetivo de classificação binária\n",
    "    params['verbose'] = False                         # Reduz a verbosidade do treinamento\n",
    "\n",
    "    # Inicialização do modelo\n",
    "    model = CatBoostClassifier(**params,cat_features=cat_features, )\n",
    "    \n",
    "    # Treinamento\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_test, y_test),\n",
    "        early_stopping_rounds=50,\n",
    "        cat_features=cat_features, \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Predições e cálculo da métrica\n",
    "    preds = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "    \n",
    "    # Retorna a métrica negativa\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "\n",
    "# Inicialização do Hyperopt\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,                     # Função objetivo\n",
    "    space=space_catboost,             # Espaço de busca\n",
    "    algo=tpe.suggest,                 # Algoritmo de busca (TPE)\n",
    "    max_evals=2,                     # Número de avaliações\n",
    "    trials=trials,                    # Armazena os resultados\n",
    "    rstate=np.random.default_rng(42)  # Reprodutibilidade\n",
    ")\n",
    "\n",
    "# Exibição dos melhores parâmetros\n",
    "print(\"Melhores parâmetros:\", best)\n",
    "\n",
    "\n",
    "# Ajuste dos Melhores Parâmetros\n",
    "best_params = {\n",
    "        'depth': int(best['depth']),  # Corrigido para \"depth\"\n",
    "        'random_strength': best['random_strength'],\n",
    "        'l2_leaf_reg': best['l2_leaf_reg'],\n",
    "        'bagging_temperature': best['bagging_temperature'],\n",
    "        'min_data_in_leaf': int(best['min_data_in_leaf']),  # Corrigido para \"min_data_in_leaf\"\n",
    "        'learning_rate': best['learning_rate'],\n",
    "        'iterations': int(best['iterations']),  # Corrigido para \"iterations\"\n",
    "        'scale_pos_weight': best['scale_pos_weight'],\n",
    "        'max_bin': int(best['max_bin']),\n",
    "        'grow_policy': ['Depthwise', 'Lossguide'][best['grow_policy']],  # Mapeia o índice para a string correta\n",
    "        'task_type': 'GPU',\n",
    "        'eval_metric': 'AUC',\n",
    "        'loss_function': 'Logloss',\n",
    "        'random_seed': 42,\n",
    "        'verbose': False }\n",
    "    \n",
    "\n",
    "# Treinamento do Modelo Final\n",
    "final_model = CatBoostClassifier(**best_params, cat_features=cat_features)\n",
    "    \n",
    "final_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_test, y_test),\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=10,\n",
    "    plot=True)\n",
    "    \n",
    "# Avaliação do Modelo Final\n",
    "final_preds = final_model.predict_proba(X_test)[:, 1]\n",
    "final_auc = roc_auc_score(y_test, final_preds)\n",
    "print(f\"AUC do modelo final: {final_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7ea9a-b429-4a7e-b169-8af27f16fa60",
   "metadata": {},
   "source": [
    "### Hipertuning Mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96751c9-5a6c-4060-8155-a3cadca96eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_mlflow(params):\n",
    "    \"\"\"\n",
    "    Realiza o ajuste de hiperparâmetros e treinamento de um modelo CatBoost com logging completo utilizando MLflow.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): Dicionário contendo os hiperparâmetros para o modelo CatBoost.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dicionário contendo o 'loss' (negativo da média do AUC) e o 'status'.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    cat_features = list(X_train_valid.select_dtypes(include=['object']))\n",
    "    \n",
    "    with mlflow.start_run(experiment_id=experiment_id, run_name='CatBoost Training and Tuning', nested=True,\n",
    "                         tags = {\"Hipertunnig\": \"Catboost\"}):\n",
    "        mlflow.log_params(params)\n",
    "        # Ajuste dos hiperparâmetros\n",
    "        params['loss_function'] = 'Logloss'           # Objetivo de classificação binária\n",
    "        params['verbose'] = False                         # Reduz a verbosidade do treinamento\n",
    "\n",
    "        # Inicialização do modelo\n",
    "        model = CatBoostClassifier(\n",
    "            **params,\n",
    "            cat_features=cat_features,\n",
    "        )\n",
    "        \n",
    "        # Treinamento\n",
    "        model.fit(\n",
    "            X_train_valid, y_train_valid,\n",
    "            eval_set=(X_test_valid, y_test_valid),\n",
    "            early_stopping_rounds=40,\n",
    "            cat_features=cat_features,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # Previsões\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "        \n",
    "        # Métricas de desempenho\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        TN, FP, FN, TP = cm.ravel()\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "        fnr = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "        g_mean = np.sqrt(sensitivity * specificity)\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall_sensibility\": recall_score(y_test, y_pred),  # Nome ajustado\n",
    "            \"f1_score\": f1_score(y_test, y_pred),\n",
    "            \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "            \"specificity\": specificity,\n",
    "            \"auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "            \"prauc\": average_precision_score(y_test, y_pred_proba),\n",
    "            \"mcc\": matthews_corrcoef(y_test, y_pred),\n",
    "            \"log_loss\": log_loss(y_test, y_pred_proba),\n",
    "            \"brier_score\": brier_score_loss(y_test, y_pred_proba),\n",
    "            \"cohen_kappa\": cohen_kappa_score(y_test, y_pred),\n",
    "            \"false_positive_rate_FPR\": fpr,  # Nome ajustado\n",
    "            \"false_negative_rate_FNR\": fnr,  # Nome ajustado\n",
    "            \"geometric_mean_GMean\": g_mean   # Nome ajustado\n",
    "        }\n",
    "\n",
    "        # Log de métricas\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "        # Gráficos e artefatos\n",
    "        output_dir = \"mlflow_artifacts\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Matriz de Confusão\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "        plt.title('Matriz de Confusão')\n",
    "        plt.xlabel('Predito')\n",
    "        plt.ylabel('Real')\n",
    "        confusion_matrix_path = os.path.join(output_dir, \"confusion_matrix.png\")\n",
    "        plt.savefig(confusion_matrix_path)\n",
    "        mlflow.log_artifact(confusion_matrix_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Curva ROC\n",
    "        fpr_vals, tpr_vals, _ = roc_curve(y_test, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr_vals, tpr_vals, linestyle='--', label='Curva ROC (AUC = {:.3f})'.format(metrics[\"auc\"]))\n",
    "        plt.title('Curva ROC')\n",
    "        plt.xlabel('Taxa de Falsos Positivos')\n",
    "        plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "        plt.legend()\n",
    "        roc_curve_path = os.path.join(output_dir, \"roc_curve.png\")\n",
    "        plt.savefig(roc_curve_path)\n",
    "        mlflow.log_artifact(roc_curve_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Curva de Precisão-Recall\n",
    "        precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(recall_vals, precision_vals, marker='.', label='PRAUC = {:.3f}'.format(metrics[\"prauc\"]))\n",
    "        plt.title('Curva de Precisão-Recall')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precisão')\n",
    "        plt.legend()\n",
    "        pr_curve_path = os.path.join(output_dir, \"precision_recall_curve.png\")\n",
    "        plt.savefig(pr_curve_path)\n",
    "        mlflow.log_artifact(pr_curve_path)\n",
    "        plt.close()\n",
    "\n",
    "        # SHAP Importance\n",
    "        explainer = shap.Explainer(model)\n",
    "        shap_values = explainer(X_test)\n",
    "        shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "        sorted_idx = shap_importance.argsort()\n",
    "\n",
    "        # Gráfico de importância SHAP\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        plt.barh(range(len(sorted_idx)), shap_importance[sorted_idx], align='center')\n",
    "        plt.yticks(range(len(sorted_idx)), X_test.columns[sorted_idx])\n",
    "        plt.title('SHAP Importance')\n",
    "        plt.tight_layout()\n",
    "        shap_importance_path = os.path.join(output_dir, \"shap_importance.png\")\n",
    "        plt.savefig(shap_importance_path)\n",
    "        mlflow.log_artifact(shap_importance_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Beeswarm SHAP\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.plots.beeswarm(shap_values, max_display=15, show=False)\n",
    "        plt.title('SHAP Beeswarm')\n",
    "        plt.tight_layout()\n",
    "        shap_beeswarm_path = os.path.join(output_dir, \"shap_beeswarm.png\")\n",
    "        plt.savefig(shap_beeswarm_path)\n",
    "        mlflow.log_artifact(shap_beeswarm_path)\n",
    "        plt.close()\n",
    "\n",
    "        # Registrar o modelo no MLflow\n",
    "        signature = infer_signature(X_test, y_pred_proba)\n",
    "        mlflow.catboost.log_model(\n",
    "            model,\n",
    "            name=\"model_catboost\",\n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "    # Retorna a métrica de perda para o Hyperopt\n",
    "    return {'loss': -metrics['auc'], 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# Espaço de Busca para o Hyperopt\n",
    "space_catboost = {\n",
    "    'iterations': scope.int(hp.quniform('iterations', 100, 1000, 50)),  # Número de árvores\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, -0.3),          # Taxa de aprendizado\n",
    "    'depth': scope.int(hp.quniform('depth', 4, 12, 1)),                 # Profundidade das árvores\n",
    "    'l2_leaf_reg': hp.loguniform('l2_leaf_reg', -3, 2),                 # Regularização L2\n",
    "    'random_strength': hp.uniform('random_strength', 0, 2),             # Aleatoriedade nas divisões\n",
    "    'bagging_temperature': hp.uniform('bagging_temperature', 0, 1),     # Temperatura do bagging\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 4, 8),           # Peso para classes desbalanceadas\n",
    "    'min_data_in_leaf': scope.int(hp.quniform('min_data_in_leaf', 10, 100, 10)),  # Mínimo de dados por folha\n",
    "    'max_bin': scope.int(hp.quniform('max_bin', 128, 256, 32)),         # Número máximo de bins\n",
    "    'grow_policy': hp.choice('grow_policy', ['Depthwise', 'Lossguide']),  # Política de crescimento\n",
    "    'eval_metric': 'AUC',\n",
    "    'task_type': 'GPU',                                                 # Utilizar GPU\n",
    "    'random_seed': 42                                                   # Reprodutibilidade\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b8c3e-6dd4-495e-8e5e-7d6b4a709b98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Etapa de hipertunning\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name='Hipertunnig', nested=True,  \n",
    "                      description = 'Busca pelos melhores parametros. Os modelos testados são armazenados, mesmo que não tenha os melhores parametros. CatBoost',\n",
    "                      tags = {\"Execução do Hipert\": \"Melhores parametros\", \"objetivo\": \"garantir os melhores parametros para o modelo\", \"Versão da etapa\": \"1.0\"}):\n",
    "\n",
    "    cat_features = list(X_train_valid.select_dtypes(include=['object']))\n",
    "    # Inicialização do Hyperopt\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective_mlflow,                     # Função objetivo\n",
    "        space=space_catboost,             # Espaço de busca\n",
    "        algo=tpe.suggest,                 # Algoritmo de busca (TPE)\n",
    "        max_evals=50,                     # Número de avaliações\n",
    "        trials=trials,                    # Armazena os resultados\n",
    "        rstate=np.random.default_rng(42)  # Reprodutibilidade\n",
    "    )\n",
    "    \n",
    "   \n",
    "    # Obtendo os melhores hiperparâmetros\n",
    "    mlflow.log_params(best)\n",
    "    print(\"Melhores hiperparâmetros:\", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081a0cfb-183d-4136-a8de-d868fc8444c6",
   "metadata": {},
   "source": [
    "## Treinamento após hipertunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3d320-e1ef-46d5-b685-742316a6dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar rastreamento MLflow\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name='Treinamento do melhor modelo modelo CatBoost', nested=True,\n",
    "                     description='Treinando o CatBoost com os melhores parametros',\n",
    "                     tags={\"Versão do modelo\": \"1\", \"Algoritmo\": \"CatBoost\"}):\n",
    "\n",
    "    # Ajuste dos Melhores Parâmetros\n",
    "    best_params = {\n",
    "        'depth': int(best['depth']),\n",
    "        'random_strength': best['random_strength'],\n",
    "        'l2_leaf_reg': best['l2_leaf_reg'],\n",
    "        'bagging_temperature': best['bagging_temperature'],\n",
    "        'min_data_in_leaf': int(best['min_data_in_leaf']),\n",
    "        'learning_rate': best['learning_rate'],\n",
    "        'iterations': int(best['iterations']),\n",
    "        'scale_pos_weight': best['scale_pos_weight'],\n",
    "        'max_bin': int(best['max_bin']),\n",
    "        'grow_policy': ['Depthwise', 'Lossguide'][best['grow_policy']],\n",
    "        'task_type': 'GPU',\n",
    "        'eval_metric': 'AUC',\n",
    "        'loss_function': 'Logloss',\n",
    "        'random_seed': 42,\n",
    "        'verbose': False\n",
    "    }\n",
    "    mlflow.log_params(best_params)\n",
    "    cat_features = list(X_train_valid.select_dtypes(include=['object']))\n",
    "    # Treinamento do Modelo Final\n",
    "    final_model = CatBoostClassifier(**best_params, cat_features=cat_features)\n",
    "\n",
    "    final_model.fit(\n",
    "        X_train_valid, y_train_valid,\n",
    "        eval_set=(X_test_valid, y_test_valid),\n",
    "        early_stopping_rounds=40,\n",
    "        verbose=10,\n",
    "        plot=True\n",
    "    )\n",
    "\n",
    "    # Avaliação do Modelo Final\n",
    "\n",
    "    # Previsões\n",
    "    y_pred_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "    # Métricas de desempenho\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "    fnr = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "    g_mean = np.sqrt(sensitivity * specificity)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall_sensitivity\": recall_score(y_test, y_pred),  # Nome ajustado\n",
    "        \"f1_score\": f1_score(y_test, y_pred),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(y_test, y_pred),\n",
    "        \"specificity\": specificity,\n",
    "        \"auc\": roc_auc_score(y_test, y_pred_proba),\n",
    "        \"prauc\": average_precision_score(y_test, y_pred_proba),\n",
    "        \"mcc\": matthews_corrcoef(y_test, y_pred),\n",
    "        \"log_loss\": log_loss(y_test, y_pred_proba),\n",
    "        \"brier_score\": brier_score_loss(y_test, y_pred_proba),\n",
    "        \"cohen_kappa\": cohen_kappa_score(y_test, y_pred),\n",
    "        \"false_positive_rate_FPR\": fpr,  # Nome ajustado\n",
    "        \"false_negative_rate_FNR\": fnr,  # Nome ajustado\n",
    "        \"geometric_mean_GMean\": g_mean   # Nome ajustado\n",
    "    }\n",
    "\n",
    "    # Log de métricas individualmente\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        mlflow.log_metric(metric_name, metric_value)\n",
    "\n",
    "    # Gráficos e artefatos\n",
    "    # Matriz de Confusão\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    mlflow.log_artifact('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Curva ROC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, linestyle='--', label='Curva ROC (AUC = {:.3f})'.format(metrics[\"auc\"]))\n",
    "    plt.title('Curva ROC')\n",
    "    plt.xlabel('Taxa de Falsos Positivos')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "    plt.legend()\n",
    "    plt.savefig('roc_curve.png')\n",
    "    mlflow.log_artifact('roc_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Curva de Precisão-Recall\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall_vals, precision_vals, marker='.', label='PRAUC = {:.3f}'.format(metrics[\"prauc\"]))\n",
    "    plt.title('Curva de Precisão-Recall')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precisão')\n",
    "    plt.legend()\n",
    "    plt.savefig('precision_recall_curve.png')\n",
    "    mlflow.log_artifact('precision_recall_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "    # SHAP Importance\n",
    "    explainer = shap.Explainer(final_model)\n",
    "    shap_values = explainer(X_test)\n",
    "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    sorted_idx = shap_importance.argsort()\n",
    "\n",
    "    # Gráfico de importância SHAP\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.barh(range(len(sorted_idx)), shap_importance[sorted_idx], align='center')\n",
    "    plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])\n",
    "    plt.title('SHAP Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_importance.png')\n",
    "    mlflow.log_artifact('shap_importance.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Beeswarm SHAP\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.plots.beeswarm(shap_values, max_display=15, show=False)\n",
    "    plt.title('SHAP Beeswarm')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_beeswarm.png')\n",
    "    mlflow.log_artifact('shap_beeswarm.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Registrar o modelo no MLflow\n",
    "    signature = infer_signature(X_test, y_pred_proba)\n",
    "    mlflow.catboost.log_model(\n",
    "        final_model,\n",
    "        artifact_path=\"model_catboost_final\",\n",
    "        signature=signature\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9bb3f-4296-43e5-96d3-93bcc3a3f9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358734da-a805-41a2-b461-791b8e834d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
